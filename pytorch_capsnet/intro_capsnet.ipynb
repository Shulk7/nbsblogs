{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to CapsNet\n",
    "\n",
    "My working implementation of capsnet from the original paper: https://arxiv.org/pdf/1710.09829.pdf by sara sabour, nick frosst and geoff hinton.\n",
    "\n",
    "My goal is to understand the dynamic routing algorithm, implement it for the simplest capsnet. I want an implementation that is understandable to people in a mid-tier skill level. I think we can learn a lot by visualizing and examining what capsnet does at each layer so that's what I'm going try to do in this notebook.\n",
    "\n",
    "Architecture:\n",
    "\n",
    "Input image -> ReLU Conv -> Primary \"Caps\" -> Digit Caps -> vector norm\n",
    "\n",
    "The new things here are the primary caps which are just conv layers whose output is passed through the squash function (instead of relu) to prepare them for the actual caps, and the digit caps which are a layer of actual capsules. The nonlinearity (squash) we use on the primary caps and the digit caps is also new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, '0.2.0_4')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_variable(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_img = os.path.join('/scratch', 'yns207', 'imgs', 'mnist')\n",
    "mnist_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                     ])\n",
    "mnist_dataset = datasets.MNIST(mnist_img, \n",
    "                               download=True, \n",
    "                               transform=mnist_transform)\n",
    "mnist_loader = data.DataLoader(dataset=mnist_dataset,\n",
    "                               batch_size=64,\n",
    "                               shuffle=True,\n",
    "                               num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to understand what goes into the capsules. Let's make a network up to the primary caps and see what the output of the previous layer is supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrimaryCapsIn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrimaryCapsIn, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.relu(self.conv(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the paper thes is a convolutional layer with kernel size  and stride 1, with 1 black/white input channel for balck white mnist and 256 output channels (or feature maps) in the conv layer. Let's now load the data and observe the shape on the other end of this layer. This should be a gentle intro to the capsnet as most people interested in this will already be familiar with convolutional nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_caps_in = PrimaryCapsIn()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    primary_caps_in.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "for epoch in range(n_epochs):\n",
    "    for image_batch in mnist_loader:\n",
    "        # get the actual batch, toss metadata\n",
    "        image_batch = image_batch[0]\n",
    "        # get the actual size of this batch\n",
    "        # for uneven datasets it may not be 64\n",
    "        # on the last batch\n",
    "        batch_size = image_batch.shape[0]\n",
    "        # convert to pytorch variable\n",
    "        image_batch = to_variable(image_batch)\n",
    "        # plug through our network\n",
    "        outputs = primary_caps_in(image_batch)\n",
    "        print(outputs.data.shape)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the input coming out of our  network up to this point is a batch of 64 tensors (for 64 images). Where each tensor is 256x20x20. This matches up with what the paper tells us to expect on page 3-4. Let's create our primary caps now. These are conv layers BUT they will turn outputs trhough the squash function and output vectors, vectors that can be fed into the actual capsules the digitcapsules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        # this creates a layer with 8 convolutions it's \n",
    "        # a normal conv layer just, 8 timse repeated\n",
    "        caps = [nn.Conv2d(in_channels=256, out_channels=32, kernel_size=9, stride=2, padding=0) for _ in range(8)]\n",
    "        self.caps = nn.ModuleList(caps)\n",
    "        \n",
    "    def squash(self, s):\n",
    "        # batch_size x all_data x 8, sum over all 8 caps, maintain shape\n",
    "        squared_norm = torch.sum(torch.pow(s, 2), dim=2, keepdim=True)\n",
    "        norm = torch.sqrt(squared_norm)\n",
    "        return (squared_norm / (1 + squared_norm)) * (s / norm)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # for every capsule in the layer\n",
    "        # plug in the inputs\n",
    "        u = [cap(inputs) for cap in self.caps]\n",
    "        # list of 8 64x32x6x6 -> tensor of 64x8x32x6x6\n",
    "        u = torch.stack(u, dim=1)\n",
    "        # flatten the outputs of the 8 capsules\n",
    "        u = u.view(u.size(0), 8, -1)\n",
    "        # apply non linearity\n",
    "        u = self.squash(u)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_caps_in = PrimaryCapsIn()\n",
    "primary_caps = PrimaryCaps()\n",
    "if torch.cuda.is_available():\n",
    "    primary_caps.cuda()\n",
    "    primary_caps_in.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 1152])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for image_batch in mnist_loader:\n",
    "        image_batch = image_batch[0]\n",
    "        batch_size = image_batch.shape[0]\n",
    "        image_batch = to_variable(image_batch)\n",
    "        outputs = primary_caps_in(image_batch)\n",
    "        outputs = primary_caps(outputs)\n",
    "        print(outputs.size())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 8 'primary' caps and they each output what is essentially a fleattened vector of all the convolutional outputs for all images in a batch.\n",
    "\n",
    "For one image we get the output of all 8 caps as 1152 features, which is equal to 32 x 6 x 6, which is what the paper tells us to expect at this point on page 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the digit caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitCaps, self).__init__()\n",
    "        # number of primary caps\n",
    "        self.n_primary_caps = 8\n",
    "        # number of outputs per primary cap\n",
    "        self.n_primary_outputs = 32 * 6 * 6\n",
    "        # number of digit caps\n",
    "        self.n_caps = 10\n",
    "        # number of outputs per digit cap\n",
    "        self.n_caps_outputs = 16\n",
    "        self.W = nn.Parameter(torch.randn(1, \n",
    "                              self.n_primary_outputs, \n",
    "                              self.n_caps, \n",
    "                              self.n_caps_outputs,\n",
    "                              self.n_primary_caps))\n",
    "        \n",
    "    def squash(self, s):\n",
    "        # v=batch_size x all_data x 8, sum over all 8 caps, maintain shape\n",
    "        squared_norm = torch.sum(torch.pow(s, 2), dim=2, keepdim=True)\n",
    "        norm = torch.sqrt(squared_norm)\n",
    "        return (squared_norm / (1 + squared_norm)) * (s / norm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## --- multiply vectors by weights\n",
    "        batch_size = x.size(0)\n",
    "        x = x.transpose(1, 2)\n",
    "        # batch, features, in_units\n",
    "        \n",
    "        x = torch.stack([x] * self.n_caps, dim=2).unsqueeze(4)\n",
    "        # batch, features, num_units, in_units, 1    \n",
    "        \n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        # batch, features, in_units, unit_size, num_units\n",
    "\n",
    "        u_hat = torch.matmul(W, x)\n",
    "        # batch, features, num_units, unit_size, 1\n",
    "\n",
    "        ## --- route the vectors\n",
    "        b = to_variable(torch.zeros(1, self.n_primary_outputs, self.n_caps, 1))\n",
    "        \n",
    "        for iteration in range(3):\n",
    "            c = F.softmax(b)\n",
    "            c = torch.cat([c]* batch_size, dim=0).unsqueeze(4)\n",
    "                        \n",
    "            # apply routing to weighted inputs u_hat\n",
    "            # then sum it together.\n",
    "            s = (c * u_hat).sum(dim=1, keepdim=True)\n",
    "                        \n",
    "            # squash the output.\n",
    "            # bathc_size, 1, n_caps, unit_size, 1\n",
    "            v = self.squash(s)\n",
    "                    \n",
    "            # batch_size, feat, n_caps, unit_size, 1\n",
    "            v_j = torch.cat([v] * self.n_primary_outputs, dim=1)\n",
    "                        \n",
    "            # 1, feat, n_caps, 1\n",
    "            u_v = torch.matmul(u_hat.transpose(3,4), v_j).squeeze(4).mean(dim=0, keepdim=True)\n",
    "                        \n",
    "            # update route vectors\n",
    "            b = b + u_v\n",
    "        return v.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_caps_in = PrimaryCapsIn()\n",
    "primary_caps = PrimaryCaps()\n",
    "digit_caps = DigitCaps()\n",
    "if torch.cuda.is_available():\n",
    "    primary_caps_in.cuda()\n",
    "    primary_caps.cuda()\n",
    "    digit_caps.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for image_batch in mnist_loader:\n",
    "        image_batch = image_batch[0]\n",
    "        batch_size = image_batch.shape[0]\n",
    "        image_batch = to_variable(image_batch)\n",
    "        outputs = primary_caps_in(image_batch)\n",
    "        outputs = primary_caps(outputs)\n",
    "        outputs = digit_caps(outputs)\n",
    "        print(outputs.size())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output makes sense. It lists for all images in the batch, the digitcaps, and the 16 outputs for that cap. Next we'll wan't to create a capsule loss to measure the loss in this case. In the paper something called a 'margin loss' is used. See formula 4 on page 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def margin_loss(predictions, labels):\n",
    "    batch_size = predictions.size(0)\n",
    "    zero = to_variable(torch.zeros(1))\n",
    "    pred_norm = torch.sqrt((predictions**2).sum( dim=2, keepdim=True))\n",
    "    max_left = torch.max(zero, 0.9 - pred_norm).view(batch_size, -1)**2\n",
    "    max_right = torch.max(zero, pred_norm - 0.1).view(batch_size, -1)**2\n",
    "    loss = labels*max_left + 0.5*(1.0 - labels)*max_right\n",
    "    margin_loss = loss.sum(dim=1).mean()\n",
    "    return margin_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine everything into one network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        \n",
    "        self.conv = PrimaryCapsIn()\n",
    "        self.primary = PrimaryCaps()\n",
    "        self.digits = DigitCaps()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.digits(self.primary(self.conv(inputs)))\n",
    "    \n",
    "    def loss(self, inputs, labels):\n",
    "        return self.margin_loss(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "one_hot_labels = to_variable(torch.eye(10))\n",
    "\n",
    "capsnet = CapsNet()\n",
    "optimizer = Adam(capsnet.parameters(), lr=lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    capsnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.06457728147506714\n",
      "epoch 1 loss: 0.018490303307771683\n",
      "epoch 2 loss: 0.035305529832839966\n",
      "epoch 3 loss: 0.009151224978268147\n",
      "epoch 4 loss: 0.01574857160449028\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for images, labels in mnist_loader:\n",
    "        mnist_labels = torch.LongTensor(labels).cuda()\n",
    "        image_batch = images\n",
    "        batch_size = images.shape[0]\n",
    "        image_batch = to_variable(image_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = capsnet(image_batch)\n",
    "\n",
    "        batch_labels = one_hot_labels.index_select(dim=0, index=mnist_labels)\n",
    "        loss = margin_loss(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('epoch {} loss: {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a train loss near 0.01 it's safe to say we have trained/fit the model! What's next?\n",
    "\n",
    "1 - Experimenting with the loss function (graphing it).\n",
    "\n",
    "2 - Implementing reconstruction. \n",
    "\n",
    "3 - Trying some loss functions like the clustering bceloss that might naturally be good for capsnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization using reconstruction\n",
    "Regularization is important to avoid overfitting and to help our netowrk learn a more general representation. They use a decoder to reconstruct inputs from the capsules and then compare those reconstructed images against the acutal images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*10, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.squeeze().transpose(0,1)\n",
    "        outputs = self.fc1(inputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.fc2(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.fc3(outputs)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_caps_in = PrimaryCapsIn()\n",
    "primary_caps = PrimaryCaps()\n",
    "digit_caps = DigitCaps()\n",
    "decoder = Decoder()\n",
    "if torch.cuda.is_available():\n",
    "    primary_caps_in.cuda()\n",
    "    primary_caps.cuda()\n",
    "    digit_caps.cuda()\n",
    "    decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for image_batch in mnist_loader:\n",
    "        image_batch = image_batch[0]\n",
    "        batch_size = image_batch.shape[0]\n",
    "        image_batch = to_variable(image_batch)\n",
    "        outputs = primary_caps_in(image_batch)\n",
    "        outputs = primary_caps(outputs)\n",
    "        outputs = digit_caps(outputs)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Other sources snippets:\n",
    "\n",
    "https://github.com/timomernick/pytorch-capsule/blob/master/capsule_network.py\n",
    "\n",
    "https://github.com/gram-ai/capsule-networks\n",
    "    \n",
    "https://github.com/XifengGuo/CapsNet-Keras\n",
    "    \n",
    "https://github.com/naturomics/CapsNet-Tensorflow/blob/master/capsNet.py\n",
    "\n",
    "https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b\n",
    "\n",
    "https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66\n",
    "\n",
    "https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418\n",
    "\n",
    "concepts to understand...\n",
    "\n",
    "Squashing function:\n",
    "\n",
    "http://mathworld.wolfram.com/VectorNorm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
