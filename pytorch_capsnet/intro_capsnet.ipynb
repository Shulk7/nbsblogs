{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to CapsNet\n",
    "\n",
    "My implementation of capsnet from the original paper: https://arxiv.org/pdf/1710.09829.pdf by sara sabour, nick frosst and geoff hinton.\n",
    "\n",
    "My goal is to understand the dynamic routing algorithm, implement it for the simplest capsnet. I want an implementation that is understandable to people in a mid-tier skill level. I think we can learn a lot by visualizing and examinign what capsnet does at each layer so that's what I'm going to do in this notebook. In the first section values will be hardcoded. In the after section we'll productionize our capsnet architecture a bit to make things a bit more flexible.\n",
    "\n",
    "Architecture:\n",
    "\n",
    "Input image -> ReLU Conv -> Primary \"Caps\" -> Digit Caps -> vector norm\n",
    "\n",
    "The new things here is the primary caps which are just conv layers whose output is passed through the squash function (instead of relu) to prepare them for the actual caps, the digit caps which are a layer of actual capsules.\n",
    "\n",
    "There are 3 dense layers at the end, one fully connected relu, another fully conected relu, and a fully connected sigmoid.\n",
    "\n",
    "A Quick note:\n",
    "\n",
    "CapsNet is brand new (1 month old) if you understand why something works, or you discover the reason it works. Let people know. Write a blog, or reach out to me and we can write one together. Everyone is still trying to understand why things work in neural nets and capsnet is no exception. Don't hoard your discoveries; they could improve the world. The barrier to discovery is very low, you could make the next great finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, '0.2.0_4')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_variable(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_img = os.path.join('/scratch', 'yns207', 'imgs', 'mnist')\n",
    "mnist_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                     ])\n",
    "mnist_dataset = datasets.MNIST(mnist_img, \n",
    "                               download=True, \n",
    "                               transform=mnist_transform)\n",
    "mnist_loader = data.DataLoader(dataset=mnist_dataset,\n",
    "                               batch_size=64,\n",
    "                               shuffle=True,\n",
    "                               num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to understand what goes into the capsules. Let's make a network up to the primary caps and see what the output of the previous layer is supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrimaryCapsIn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrimaryCapsIn, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.relu(self.conv(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the paper thes is a convolutional layer with kernel size  and stride 1, with 1 black/white input channel for balck white mnist and 256 output channels (or feature maps) in the conv layer. let's now laod the data and observe the shape on the other end of this layer. This should be a gentle intro to the capsnet as most people interested in this will already be familiar with convolutional nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_caps_in = PrimaryCapsIn()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    primary_caps_in.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "for epoch in range(n_epochs):\n",
    "    for image_batch in mnist_loader:\n",
    "        # get the actual batch, toss metadata\n",
    "        image_batch = image_batch[0]\n",
    "        # get the actual size of this batch\n",
    "        # for uneven datasets it may not be 64\n",
    "        # on the last batch\n",
    "        batch_size = image_batch.shape[0]\n",
    "        # convert to pytorch variable\n",
    "        image_batch = to_variable(image_batch)\n",
    "        # plug through our network\n",
    "        outputs = primary_caps_in(image_batch)\n",
    "        print(outputs.data.shape)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the input coming out of our  network up to this point is a batch of 64 tensors (for 64 images). Where each tensor is 256x20x20. This matches up with what the paper teels us to expect on page 3-4. Let's create our primary caps now. These are conv layers BUT they will turn outputs into a vector. afterwards. A vector that can be fed into the actual capsules the digitcapsules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "        # this creates a layer with 8 conv2d \n",
    "        caps = [nn.Conv2d(in_channels=256, out_channels=32, kernel_size=9, stride=2, padding=0) for _ in range(8)]\n",
    "        self.caps = nn.ModuleList(caps)\n",
    "        \n",
    "    def squash(self, s):\n",
    "        # s shape batch_size x all_data x 8, sum over all 8 caps, maintain shape\n",
    "        squared_norm = torch.sum(torch.pow(s, 2), dim=2, keepdim=True)\n",
    "        norm = torch.sqrt(squared_norm)\n",
    "        return (squared_norm / (1 + squared_norm)) * (s / norm)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # for every capsule in the layer\n",
    "        # plug in the inputs\n",
    "        u = [cap(inputs) for cap in self.caps]\n",
    "        # list of 8 64x32x6x6 -> tensor of 64x8x32x6x6\n",
    "        u = torch.stack(u, dim=1)\n",
    "        #flatten the outputs\n",
    "        u = u.view(u.size(0), 8, -1)\n",
    "        # apply non linearity\n",
    "        u = self.squash(u)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_caps_in = PrimaryCapsIn()\n",
    "primary_caps = PrimaryCaps()\n",
    "if torch.cuda.is_available():\n",
    "    primary_caps.cuda()\n",
    "    primary_caps_in.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 1152])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for image_batch in mnist_loader:\n",
    "        image_batch = image_batch[0]\n",
    "        batch_size = image_batch.shape[0]\n",
    "        image_batch = to_variable(image_batch)\n",
    "        outputs = primary_caps_in(image_batch)\n",
    "        outputs = primary_caps(outputs)\n",
    "        print(outputs.size())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 8 'primary' caps and they each output what is essentially a fleattened vector of all the convolutional outputs for all images in a batch (just read the tensor shaep backwards as is the convention in pytorch). Let's look at the actual data in this tensor...\n",
    "\n",
    "Ok so it looks like for one image we get the output of all 8 caps, which is equal to 32 x 6 x 6, which is what the paper tells us to expect at this point on page 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the digit caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitCaps, self).__init__()\n",
    "        self.in_units = 8 # number of primary caps\n",
    "        self.in_channels = 32 * 6 * 6 # number of outputs per primary cap\n",
    "        self.num_units = 10 # 10 digit caps\n",
    "        self.unit_size = 16 # number of channels/digit cap\n",
    "        self.W = nn.Parameter(torch.randn(1, \n",
    "                                                self.in_channels, \n",
    "                                                self.num_units, \n",
    "                                                self.unit_size,\n",
    "                                                self.in_units))\n",
    "        \n",
    "    def squash(self, s):\n",
    "        # s shape batch_size x all_data x 8, sum over all 8 caps, maintain shape\n",
    "        squared_norm = torch.sum(torch.pow(s, 2), dim=2, keepdim=True)\n",
    "        norm = torch.sqrt(squared_norm)\n",
    "        return (squared_norm / (1 + squared_norm)) * (s / norm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## --- multiply vectors by weights\n",
    "        batch_size = x.size(0)\n",
    "        x = x.transpose(1, 2)\n",
    "        # batch, features, in_units\n",
    "        \n",
    "        x = torch.stack([x] * self.num_units, dim=2).unsqueeze(4)\n",
    "        # batch, features, num_units, in_units, 1    \n",
    "        \n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        # batch, features, in_units, unit_size, num_units\n",
    "\n",
    "        u_hat = torch.matmul(W, x)\n",
    "        # batch, features, num_units, unit_size, 1\n",
    "\n",
    "        ## --- route the vectors\n",
    "        b = Variable(torch.zeros(1, self.in_channels, self.num_units, 1)).cuda()\n",
    "        \n",
    "        for iteration in range(3):\n",
    "            c = F.softmax(b)\n",
    "            c = torch.cat([c]* batch_size, dim=0).unsqueeze(4)\n",
    "                        \n",
    "            # apply routing to weighted inputs u_hat\n",
    "            # then sum it together.\n",
    "            s = (c * u_hat).sum(dim=1, keepdim=True)\n",
    "                        \n",
    "            # squash the output.\n",
    "            # bathc_size, 1, n_caps, unit_size, 1\n",
    "            v = self.squash(s)\n",
    "                    \n",
    "            # batch_size, feat, n_caps, unit_size, 1\n",
    "            v_j = torch.cat([v] * self.in_channels, dim=1)\n",
    "                        \n",
    "            # 1, feat, n_caps, 1\n",
    "            u_v = torch.matmul(u_hat.transpose(3,4), v_j).squeeze(4).mean(dim=0, keepdim=True)\n",
    "                        \n",
    "            # update route vectors\n",
    "            b = b + u_v\n",
    "        return v.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_caps_in = PrimaryCapsIn()\n",
    "primary_caps = PrimaryCaps()\n",
    "digit_caps = DigitCaps()\n",
    "if torch.cuda.is_available():\n",
    "    primary_caps_in.cuda()\n",
    "    primary_caps.cuda()\n",
    "    digit_caps.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for image_batch in mnist_loader:\n",
    "        image_batch = image_batch[0]\n",
    "        batch_size = image_batch.shape[0]\n",
    "        image_batch = to_variable(image_batch)\n",
    "        outputs = primary_caps_in(image_batch)\n",
    "        outputs = primary_caps(outputs)\n",
    "        outputs = digit_caps(outputs)\n",
    "        print(outputs.size())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output makes sense. It lists for all batches, the 10 digit caps and the 16 channels in each deget cap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].squeeze().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above contains the logits for all 10 caps for all 16 channels in that cap for one image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll wan't to create a capsule loss to measure the loss in this case. In the paper something called a 'margin loss' is used. See formula 4 on page 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def margin_loss(predictions, labels):\n",
    "    batch_size = predictions.size(0)\n",
    "    zero = to_variable(torch.zeros(1))\n",
    "    pred_norm = torch.sqrt((predictions**2).sum( dim=2, keepdim=True))\n",
    "    max_left = torch.max(zero, 0.9 - pred_norm).view(batch_size, -1)**2\n",
    "    max_right = torch.max(zero, pred_norm - 0.1).view(batch_size, -1)**2\n",
    "    loss = labels*max_left + 0.5*(1.0 - labels)*max_right\n",
    "    margin_loss = loss.sum(dim=1).mean()\n",
    "    return margin_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine everything into one network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        \n",
    "        self.conv = PrimaryCapsIn()\n",
    "        self.primary = PrimaryCaps()\n",
    "        self.digits = DigitCaps()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.digits(self.primary(self.conv(inputs)))\n",
    "    \n",
    "    def loss(self, inputs, labels):\n",
    "        return self.margin_loss(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "lr = 0.01\n",
    "one_hot_labels = to_variable(torch.eye(10))\n",
    "\n",
    "capsnet = CapsNet()\n",
    "optimizer = Adam(capsnet.parameters(), lr=lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    capsnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.06106878072023392\n",
      "epoch 1 loss: 0.048292066901922226\n",
      "epoch 2 loss: 0.02802092768251896\n",
      "epoch 3 loss: 0.013605736196041107\n",
      "epoch 4 loss: 0.037176355719566345\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for images, labels in mnist_loader:\n",
    "        mnist_labels = torch.LongTensor(labels).cuda()\n",
    "        image_batch = images\n",
    "        batch_size = images.shape[0]\n",
    "        image_batch = to_variable(image_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = capsnet(image_batch)\n",
    "\n",
    "        batch_labels = one_hot_labels.index_select(dim=0, index=mnist_labels)\n",
    "        loss = margin_loss(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('epoch {} loss: {}'.format(epoch, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a train loss near 0.01 it's safe to say we have trained/fit the model!\n",
    "\n",
    "Regularization is important to avoid overfitting and to help our netowrk learn a more general representation. They use a decoder to reconstruct inputs from the capsules and then compare those reconstructed images against the acutal images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*10, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.squeeze().transpose(0,1)\n",
    "        outputs = self.fc1(inputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.fc2(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.fc3(outputs)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primary_caps_in = PrimaryCapsIn()\n",
    "primary_caps = PrimaryCaps()\n",
    "digit_caps = DigitCaps()\n",
    "decoder = Decoder()\n",
    "if torch.cuda.is_available():\n",
    "    primary_caps_in.cuda()\n",
    "    primary_caps.cuda()\n",
    "    digit_caps.cuda()\n",
    "    decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for image_batch in mnist_loader:\n",
    "        image_batch = image_batch[0]\n",
    "        batch_size = image_batch.shape[0]\n",
    "        image_batch = to_variable(image_batch)\n",
    "        outputs = primary_caps_in(image_batch)\n",
    "        outputs = primary_caps(outputs)\n",
    "        outputs = digit_caps(outputs)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Other sources snippets:\n",
    "\n",
    "https://github.com/timomernick/pytorch-capsule/blob/master/capsule_network.py\n",
    "\n",
    "https://github.com/gram-ai/capsule-networks\n",
    "    \n",
    "https://github.com/XifengGuo/CapsNet-Keras\n",
    "    \n",
    "https://github.com/naturomics/CapsNet-Tensorflow/blob/master/capsNet.py\n",
    "\n",
    "https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b\n",
    "\n",
    "https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66\n",
    "\n",
    "https://medium.com/ai³-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418\n",
    "\n",
    "concepts to understand...\n",
    "\n",
    "Squashing function:\n",
    "\n",
    "http://mathworld.wolfram.com/VectorNorm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
