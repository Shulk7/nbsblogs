{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dice Probabilities\n",
    "\n",
    "given two 6-sided dice what is the probability of getting a 4, what is the robability of getting 9?\n",
    "\n",
    "**step 1 - figure out how many total ways the dice could fall.**\n",
    "\n",
    "there are $6*6 = 36$ total possibilities when rolling 2 dice. each side on each di can combine with 6 other possibilities. $(1*6) + (1*6) + (1*6) + (1*6) + (1*6) + (1*6) = 6*6$\n",
    "\n",
    "**step 2 - figure out how many ways to produce 4 and 9.**\n",
    "\n",
    "4 can be constructed in 3 ways like so:\n",
    "\n",
    "```\n",
    "1+3\n",
    "3+1\n",
    "2+2\n",
    "```\n",
    "\n",
    "so there is a $3\\div36$ or $0.0833$ chance of getting a 4.\n",
    "\n",
    "\n",
    "9 can be constructed in 7 ways like so:\n",
    "\n",
    "```\n",
    "8+1\n",
    "1+8\n",
    "7+2\n",
    "2+7\n",
    "6+3\n",
    "3+6\n",
    "5+4\n",
    "4+5\n",
    "```\n",
    "\n",
    "$7\\div36 = 0.1945$ chance of getting a 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem\n",
    "\n",
    "Imagine a test with a true positive rate of 100% and false positive rate of 5%. Imagine a population with a 1/1000 rate of having the condition the test identifies. Given a positive test, what is the probability of having that condition?\n",
    " \n",
    "you can use bayes theorem to incorporate prior inforamtion into a probabilistic estimate:\n",
    "\n",
    "$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}\n",
    "$\n",
    "\n",
    "**step 1 - state bayes theorem and refactor to get all our 'knowns' on the right.**\n",
    "\n",
    "HC - 'having condition'\n",
    "\n",
    "PT - 'positive test'\n",
    "\n",
    "$ P(HC \\mid PT) P (PT) = P(PT \\mid HC) P(HC)$\n",
    "\n",
    "$ Z = P(PT \\mid HC) * P(HC) \\div P (PT)$\n",
    "\n",
    "**step 2 - calculate all the values we need to get the unknown:**\n",
    "\n",
    "if we have the condition the probability of a positive test is 1.0. (true positive rate of 100%)\n",
    "\n",
    "$P(PT \\mid HC) = 1 $\n",
    "\n",
    "the probability of having the condition is 1/1000.\n",
    "\n",
    "$P(HC) = (1 \\div 1000) $\n",
    "\n",
    "the probability of a positive test is number_positive_tests / total people. we can use our true positive rate and false negative rate to construct this number.\n",
    "\n",
    "$P(PT) = (1*1.0+999*0.05)\\div1000$\n",
    "\n",
    "** step 3 - filling in for all values we get:**\n",
    "\n",
    "$ Z = 1 * (1 \\div 1000) \\div (1*1.0+999*0.05)\\div1000$ \n",
    "\n",
    "$Z = 1.9627085377821394e-08$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a normal distribution?\n",
    "\n",
    "What is a normal distirbution?\n",
    "\n",
    "**description:**\n",
    "\n",
    "A normal distribution is a distribution with mean $\\mu$ and standard deviation $\\sigma$.\n",
    "\n",
    "**math:**\n",
    "\n",
    "the density function fo the noraml distribution looks like so:\n",
    "\n",
    "$\\frac{e^{-x^2}}{2}$\n",
    "\n",
    "65% of data is within $1\\sigma$ of $\\mu$. 95% within $2\\sigma$ of $\\mu$. 99.7% of data within $3\\sigma$ of $\\mu$.\n",
    "\n",
    "**examples:**\n",
    "\n",
    "normal dist examples: height in the global population, global age distribution. a normal distribution:\n",
    "\n",
    "<img src=\"http://img.tfd.com/mk/D/X2604-D-41.png\"/>\n",
    "\n",
    "other sources:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Normal_distribution\n",
    "\n",
    "https://www.wolframalpha.com/input/?i=e%5E(-(x%5E2)%2F2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a long tailed distribution?\n",
    "\n",
    "**description:**\n",
    "\n",
    "A long tailed distribution is a distribution that has a thicker tail. It is an unusual type of distribution (or rather more interesting) because the most popular items account for much less than they sould. It differs from a fat tail distribution because of this last point. it sits opposed to the pareto pinciple, which says that 20% of the products account for 80% of the purchases. with a long tail the top 20% only account for about 50% of purchases; the other 30% has gone into the long tail(s).\n",
    "\n",
    "**math:**\n",
    "\n",
    "$\\frac{1}{x^{2}}$ is an example of a function that could represent a long tailed distribution. it decays very slowly and so the tail, while appearing to narrow, actually contains an abnormally large amount of the products/purchases (the y axis being purchases and x axis being products).\n",
    "\n",
    "\n",
    "**examples:**\n",
    "\n",
    "for niche products loved by hipsters, like obscure books. the long tail effect has become more pronounced thanks to the internet; more niche content is being consumed. business school professors won't shut up about it.\n",
    "\n",
    "<img src=\"http://im.ft-static.com/content/images/da586284-be34-11e2-bb35-00144feab7de.jpg\"/>\n",
    "\n",
    "other sources:\n",
    "\n",
    "https://www.youtube.com/watch?v=UD-UEiuMa8w\n",
    "\n",
    "https://en.wikipedia.org/wiki/Long_tail#Statistical_meaning\n",
    "\n",
    "https://math.stackexchange.com/questions/382553/difference-between-a-long-tail-and-normal-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a lognormal distribution?\n",
    "\n",
    "**description:**\n",
    "\n",
    "the lognormal distribution is a distribution that models many natural phnomena. unlike the normal distribution the lognormal is not symmetrical. the lognormal distribution also has a positive skew (bsecause of the log function's effect on squashing numbers close to zero), a long tail for greater positive values. the lognormal cannot go below zero (because you cannot take al og of zero). the lognormal assumes the underlying data is normally distributed.\n",
    "\n",
    "**math:**\n",
    "\n",
    "for the density function of a lognormal see: https://www.mathworks.com/help/stats/lognpdf.html?requestedDomain=www.mathworks.com\n",
    "\n",
    "**examples:**\n",
    "\n",
    "the lognormal is used often in finance because it represents the behavior of a stock price quite well. even though stock price can go down (negative returns) the stock price can never go below zero. it is also used in economics to measure income, hydrology to measure rainfall and river volumes, and many other fields ([see the wikipedia page](https://en.wikipedia.org/wiki/Log-normal_distribution#Occurrence_and_applications)).\n",
    "\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-c9d264b86045ca2e71c03dfecf781fd7\"/>\n",
    "\n",
    "other sources:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Log-normal_distribution\n",
    "\n",
    "https://en.wikipedia.org/wiki/Natural_logarithm\n",
    "\n",
    "http://www.investopedia.com/articles/investing/102014/lognormal-and-normal-distribution.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can you tell if data is not normally distributed?\n",
    "\n",
    "**description:**\n",
    "\n",
    "you can perform tests to tell whether of not your data is normally distributed. we will go over some of these below.\n",
    "\n",
    "\n",
    "**math:**\n",
    "\n",
    "use a qq (quantile-quantile) plot:\n",
    "\n",
    "a qq plot is a visual tool that helps you determine if two samples come from the same distribution. If the qqplot shows a straight line then the two samples probabl ycome from the same distribution. if the plot shows somethign else the the two samples probably come from differetn distributions. to create one you need to sort the sample data from  both distributions then calculate the quantile each data point falls into on a continuous scale (so you could be in the 1.25th quantile). then you graph each point as the 0 through nth sample from each of the sets of samples x being the quantile value of the original distribution sample, y being the quantile value of the uncertain distribution sample you are testing.\n",
    "\n",
    "two samples from different distributions:\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/Normal_exponential_qq.svg/360px-Normal_exponential_qq.svg.png\"/>\n",
    "\n",
    "two samples from the same distributions:\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Normal_normal_qq.svg/360px-Normal_normal_qq.svg.png\"/>\n",
    "\n",
    "use a hypothesis test (ks-test): \n",
    "\n",
    "use a [hypothesis test](https://medium.com/@yvanscher/hypothesis-testing-fcc5d385dc59) we can use to deterine if a hypothesis is correct or not. we se a test statistic, we calculate it and check the likelihood of getting that value for the test statistic if the distributions were the same. if the likelhood is low enough (below some threshold p) we say that the two samples come from different distributions.\n",
    "\n",
    "A kolomogorov-smirnov (aka ks) computes a test statistic and tell use the likelihood of observing that test statistic if the distributions were the same. If the probability of getting that test statistic is low enough (below some p threshold) we can say the distributions are different. So you can compare your sample data to the normal distribution to see what the probability of if being from the normal distribution is. If the probability is low enough it is safe to say the data is not normal.\n",
    "\n",
    "The ks-test works in much the same way as a qq plot works by taking cumulative distribution functions and creating a distance metric (the test statistic) between the two functions. The code below compares 10 random value samples from the normal distribution against a normal distribution with $\\mu = 0$ and $\\sigma = 1$. This should yield a high p-value (well above 5) as we sampled from the normal and we're asking our ks test if that data is normally distributed.\n",
    "\n",
    "```python\n",
    "from scipy import stats, mean\n",
    "test_pvalues = [stats.kstest(stats.norm.rvs(loc=0, scale=1,size=1000), 'norm', args=(0, 1), N=100).pvalue for i in range(10)]\n",
    "mean(test_pvalues)\n",
    "0.43744642531754119\n",
    "```\n",
    "\n",
    "There are other types of statsitical tests. We will cover some but others you should [investigate on your own](https://cyfar.org/types-statistical-tests).\n",
    "\n",
    "**examples:**\n",
    "\n",
    "What kind of data does not follow a normal or lognormal distribution?\n",
    "\n",
    "starting salaries for lawyers (lawyers out of school and seior lawyers make a lot, between those two points people make less). speed limits are also bimodal (clustered around 35 and 65 miles per hour).\n",
    "\n",
    "\n",
    "other sources:\n",
    "\n",
    "https://graphpaperdiaries.com/2016/08/28/5-examples-of-bimodal-distributions-none-of-which-are-human-height/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Q–Q_plot\n",
    "\n",
    "http://www.physics.csbsju.edu/stats/KS-test.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate a point estimate and confidence interval.\n",
    "\n",
    "A sample of size $n = 100$ produced the sample mean of $\\bar{x} = 16$. Assuming the population standard deviation $\\sigma = 3$, compute a 95% confidence interval for the population mean $\\mu$.\n",
    "\n",
    "**step 1 - states assumptions, choose sample statistic and confidence level:**\n",
    "\n",
    "we are finding the population mean $\\mu$, at a confidence level of 95%.\n",
    "\n",
    "the confidence interval is equal to:\n",
    "\n",
    "$\\bar{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "**step 2 - find the margin of error:**\n",
    "\n",
    "for a normal distribution (because n > 30) a 95% confidence interval falls at a signifigance level of 0.05 so $\\alpha = 0.05$\n",
    "\n",
    "split 0.05 so that $\\alpha/2 = 0.025$ on either side of the normal distribution; this gives us a probability (pvalue) of 0.975.\n",
    "\n",
    "the pvalue of 0.975 corresponds to a z score $z_{\\alpha/2} = 1.95996$\n",
    "\n",
    "**steps 3 - state the confidence interval:**\n",
    "\n",
    "$16\\pm (1.96 * 3 \\div \\sqrt{100})$\n",
    "\n",
    "$16 \\pm 0.588$\n",
    "\n",
    "So we are 95% confident that the true mean is in range: $[15.412, 16.588]$\n",
    "\n",
    "Confidence intervals desribe the uncertainty of the sampling method not the uncertainty of the population mean $\\mu$. This one confidence interval doesn't tell us much. So if we repeated steps 1-3 above 100 times, sampled 100 times and got 100 confidence intervals, 95 of those intervals would contain the true population mean. We also know that 95% of the time a sample mean we draw will fall in the range $[15.412, 16.588]$.\n",
    "\n",
    "other sources:\n",
    "\n",
    "http://www.utdallas.edu/~mbaron/3341/Practice12.pdf\n",
    "\n",
    "http://stattrek.com/estimation/confidence-interval.aspx\n",
    "\n",
    "https://math.stackexchange.com/questions/1394789/how-to-calculate-probability-with-z-score-not-on-table (how z scores are calculated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8 What is a t-test? chi-squared? f-test?\n",
    "\n",
    "**description:**\n",
    "\n",
    "t-test: compares two means and tells you if they are different from each other and tells you the likelihood of getting this difference could have oxxured by chance (the p-value). the t-test assumes that sample means will be noramlly distributed (or rather t-distributed). the central limit theorem 'proves' this but frankly in my experience t-tests are less robust than other alternatives like the 2 sample ks-test.\n",
    "\n",
    "chi-squared: this test tells you how well your data fits some other distribution. a low chi-squared value tells yo uthat th edata fits some other distribution closely, a high chi squared value tells you that your data does not fit a distribution well. yo ucan use a table to lookup p-values for chi-squared test statistics. you cannot use chi-squared with % or rations, make sure to use the underlying numbers or the statistic will be wrong.\n",
    "\n",
    "f-test:this test compares the chi squared variables. if the degrees of freedom are the same you can just compare variances (squared standard devation values.) the f-test helps you determine if the variances are the same or different.\n",
    "\n",
    "**math:**\n",
    "\n",
    "t-test:\n",
    "\n",
    "$t = \\frac{\\mu_{2} - \\mu_{1}}{\\sigma_{1}  /\\sqrt{n}}$\n",
    "\n",
    "chi-squared: \n",
    "\n",
    "$x_{c} = \\sum \\frac{(O_{i} - E_{i})^2}{E_{i}}$\n",
    "\n",
    "f-test:\n",
    "\n",
    "$f = \\frac{\\sigma_{2}^2 / df }{\\sigma_{1}^2 / df}$\n",
    "\n",
    "df is the degrees of freedom for this variance. \n",
    "\n",
    "**examples:**\n",
    "\n",
    "\n",
    "t-test:\n",
    "\n",
    "a t-test can be used to ask whether the effect of some teratment shifts the distribution (has some effect). below is a  distribution of t-scores over the population. it is similar to the normal distribution except when degrees of freedom (your sample size) is low.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Student_t_pdf.svg/325px-Student_t_pdf.svg.png\"/>\n",
    "\n",
    "chi-squared test:\n",
    "\n",
    "a chi squared distribution is the distribution of chi squared values over the population. it is asymetrical and non-normal towards the right (positive values). chi squared can tell you how well one distribution fits another (are two datasets coming from the same distribution?).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/35/Chi-square_pdf.svg/321px-Chi-square_pdf.svg.png\"/>\n",
    "\n",
    "f-test:\n",
    "\n",
    "an f-test can be used to ask whether or not soe treatment reduces the variance of some process (widens or narrows the distribution). below is the distribution of f scores over the population. its shape depends on the degrees of freedom of each of the underlying chi squared scores. an f-test can be used in many of the same scenarios as a t-test but where you want to examine the variance instead of the means on the t distribution. for example ANOVA uses f-tests.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/F-distribution_pdf.svg/325px-F-distribution_pdf.svg.png\"/>\n",
    "\n",
    "\n",
    "other sources:\n",
    "\n",
    "https://people.richland.edu/james/lecture/m170/ch13-f.html\n",
    "\n",
    "http://www.statisticshowto.com/probability-and-statistics/t-test/\n",
    "\n",
    "http://onlinestatbook.com/2/analysis_of_variance/multiway.html\n",
    "\n",
    "http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-analysis-of-variance-anova-and-the-f-test\n",
    "\n",
    "https://people.richland.edu/james/lecture/m170/ch13-f.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  How to perform a t-test.\n",
    "\n",
    "**desrciption:**\n",
    "\n",
    "A t-test is a test that tells you how different two means are and it tells you the likelihood of that difference occurring by random chance (signifigance).\n",
    "\n",
    "**math:**\n",
    "\n",
    "To do this we will draw samples from both data sets and then use the means to calculate a test statistic, for the t-test this is called the t-score. The t-score is compared against the t distribution (a close approximation of a normal distribution under most circumstances). if the test statistic produces an extreme/unlikely value on the t distribution we can say that the test sample mean $\\mu_{test}$ is different from $\\mu_{norm}$.\n",
    "\n",
    "**examples:**\n",
    "\n",
    "this test is often used to measure whether or not some 'treatment' has an effect. for example a researcher may hypothesize that 'stimulating' rats with electricity encourages them to eat. \n",
    "\n",
    "we design an experiment like so:\n",
    "\n",
    "$H_0: \\mu_{control} = \\mu_{stimulated}$\n",
    "\n",
    "$H_a: \\mu_{control} \\ne \\mu_{stimulated}$ \n",
    "\n",
    "if $H_0$ is true then the drug has no effect. if $H_a$ is true then the drug has some effect. we can use a t-test to see which hypothesis is true at a 0.05 signifigance level. we 'stimulate' some rats and get the following results for how much food the rats ate:\n",
    "\n",
    "```python\n",
    "# foods rats ate\n",
    "control = [8,7,4,14,6,7,12,5,5,8]\n",
    "stimulated = [12,7,3,11,8,5,14,7,9,10]\n",
    "\n",
    "# statistics calculated\n",
    "import numpy as np\n",
    "mu_control = np.mean([8,7,4,14,6,7,12,5,5,8])\n",
    "mu_stimulated = np.mean([12,7,3,11,8,5,14,7,9,10])\n",
    "\n",
    "std_control = np.std([8,7,4,14,6,7,12,5,5,8])\n",
    "std_stimulated = np.std([12,7,3,11,8,5,14,7,9,10])\n",
    "\n",
    "```\n",
    "\n",
    "$\\mu_{control}: 7.5999$\n",
    "\n",
    "$\\mu_{stimulated}: 8.5999$\n",
    "\n",
    "$\\sigma_{stimulated}: 3.1368$\n",
    "\n",
    "$n: 10$\n",
    "\n",
    "now calculate a t statistic:\n",
    "\n",
    "$t = \\frac{\\mu_{stimulated} - \\mu_{control}}{\\sigma_{stimulated}  /\\sqrt{n}}$\n",
    "\n",
    "$t = \\frac{8.5999 - 7.5999}{3.1368 / \\sqrt{10}}$\n",
    "\n",
    "$t = 1$\n",
    "\n",
    "the p-value (cumulative probability) asssociated with this t score on either side of the t-distribution is .340893. you can use a t table to get this value, apparently calculating it by hand is 'difficult' (lol plz). this is not significant at the 0.05 level (the p-value is too big) and so we can say that electrical stimulation of rats has no effect on rats eating at this signifigance level. another way of saying this is that we could have measured these number easily by chance.\n",
    "\n",
    "other sources:\n",
    "\n",
    "http://rpsychologist.com/d3/tdist/\n",
    "\n",
    "http://faculty.webster.edu/woolflm/ttest.html\n",
    "\n",
    "http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-t-tests-t-values-and-t-distributions\n",
    "\n",
    "http://www.socscistatistics.com/pvalues/tdistribution.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do an ANOVA (analysis of variance)?\n",
    "\n",
    "**description:**\n",
    "\n",
    "an analysis of variance (ANOVA) is a tool we can use to examine multiple classes to see how much variance each has within its own data and between classes.\n",
    "\n",
    "**math:**\n",
    "\n",
    "$x_i:$ a datapoint from a particular class\n",
    "\n",
    "$\\bar{\\bar{x}}:$ the mean of the means of all classes.\n",
    "\n",
    "$\\bar{x_i}:$ the mean for a particular class.\n",
    "\n",
    "$SST = \\sum(x_i - \\bar{\\bar{x}})^2$\n",
    "\n",
    "$SSW = \\sum(x_i - \\bar{x_i})^2$\n",
    "\n",
    "$SSB = \\sum(\\bar{x_i} - \\bar{\\bar{x}})^2$\n",
    "\n",
    "**examples:**\n",
    "\n",
    "you can use ANOVA to do hypothesis tests, say we have n data points for m types of food that represents test scores for students who ate that food:\n",
    "\n",
    "food 1: [3, 2, 1]\n",
    "\n",
    "food 2: [5, 3, 4]\n",
    "\n",
    "food 3: [5, 6, 7]\n",
    "\n",
    "to check to see whether the food influences test scores we make two hypotheses:\n",
    "\n",
    "$H_0: \\mu_1 = \\mu_2 = \\mu_3$ (food type has no impact on tests scores)\n",
    "\n",
    "$H_a:$ the above equality does not hold (food type does affect test scores)\n",
    "\n",
    "if the mean test score is different in a statistically significant way for one of the types food then that food type probably has some effect on test scores. to test our hypothesis le'ts use an f test statistic:\n",
    "\n",
    "$f = \\frac{SSB / df_{ssb} }{SSW / df_{ssw}}$\n",
    "\n",
    "the degrees ofr freedom of the SSB is the number of class means minus 1. the dregrees of freedom of the SSW is he number of classes times the number in each class minus 1.\n",
    "\n",
    "$df_{ssb} = m - 1$\n",
    "\n",
    "$df_{ssw} = m*(n-1)$\n",
    "\n",
    "$df_{ssb} = 3 - 1 = 2$\n",
    "\n",
    "$df_{ssw} = 3 * (3-1) = 6$ \n",
    "\n",
    "$SSB = 24$\n",
    "\n",
    "$SSW = 6$\n",
    "\n",
    "$f = \\frac{24 / 2}{6 / 6} = 12$\n",
    "\n",
    "if we look 12 up on the f-dstirbution we can see that this is a very unlikely f statistic (well beyong the 0.05 signifigance level. we should thererfore reject the original hypothesis $H_0$. the means are different we should accept the alternate hypothesis $H_a$. the food type does affect test scores.\n",
    "\n",
    "other sources: \n",
    "\n",
    "https://www.khanacademy.org/math/statistics-probability/analysis-of-variance-anova-library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is A/B testing? How is it different from hypothesis tests?\n",
    "\n",
    "**description:**\n",
    "\n",
    "An A/B test is a form of hypothesis testing where you make a prediction about state B compared to state A and see if a statistically significant difference exists. The goal is to run this test on a small sample with the result of picking whatever state, A or B, that will be better for the whole population.\n",
    "\n",
    "**example:**\n",
    "\n",
    "This technique is used to test if different product features are useful. It is also used to see if design improvements on web pages really improves user engagement.\n",
    "\n",
    "other sources:\n",
    "\n",
    "https://conversionsciences.com/blog/ab-testing-statistics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type I and Type II errors.\n",
    "\n",
    "**description:**\n",
    "\n",
    "a **type I error** is an incorrect rejection of the null hypothesis. this is related to signifigance level. the p-value (signifigance level) is the probability that you rejected the null hypothesis when it was true; the p-value is the probability of making a type I error. you want that to be as low as possible.\n",
    "\n",
    "a **type II error** is an incorrect recjection of the alternate hypothesis. statistical power is the probability of *not* making a type II error. its the probability of finding a statistically significant result. you want your statistical power to be as high as possible; you can get it higher by including more data (a bigger sample). \n",
    "\n",
    "**examples:**\n",
    "\n",
    "if you had two hypothesis:\n",
    "\n",
    "$H_{0}: \\mu_{drug} = \\mu_{placebo}$\n",
    "\n",
    "$H_{a}: \\mu_{drug} \\ne \\mu_{placebo}$\n",
    "\n",
    "and you got a p-value of 0.1 for your test statistic on the placebo data that would mean you would have a 10% chance of rejecting the null hypothesis when it is actually the correct one. you would have a 10% chance of making a type I error.\n",
    "\n",
    "Calculating statistical power is usually done on a computer and so we can't write out a short example here.\n",
    "\n",
    "other sources:\n",
    "\n",
    "https://conversionsciences.com/blog/ab-testing-statistics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision and Recall\n",
    "\n",
    "**description:**\n",
    "\n",
    "precision (aka the true positive rate) tells you how good your model is at selecting positive elements. it punishes you for having lots of false positives (things you predicted + but that were actually -). recall (aka the sensitivity ) tells you how good your model is at actually finding all the positive examples. it punishes you for having lots of false negatives (thingsy ou predicted were - but were actually +). some quick teriminology:\n",
    "\n",
    "porisitve class: things that have the label 1.0 (for example 1.0 could represent a patient that has cancer)\n",
    "negative class: things that have the label 0.0 (for example 0.0 could represent a cancer free patient)\n",
    "\n",
    "**examples:**\n",
    "\n",
    "$TP$: true positives\n",
    "\n",
    "$FP$: false positives\n",
    "\n",
    "$FN$: false negatives\n",
    "\n",
    "$precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "$recall = \\frac{TP}{TP+FN}$\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\"/>\n",
    "\n",
    "a model that has high recall is good at grabbing lots of relevant things, a recall of one says \"out of all the data points that have the label i am searching for i predicted most of them to have the thing i am seach for.\" if you predicted everything to be the class you are searching for then you would have a perfect recall because you got every example of the thing you were searching for. having a good recall means that your model is capable of casting a wide net and getting all the things you are looking for. a model that has high precision asks whether or not the stuff your model selected is actually the correct class. so if you collected a lot of junk to boost your recall this would hurt your precision.\n",
    "\n",
    "to further reiterate, recall measures how much of the prositive class your model got (the more your model captures the better). precision measures how much of the stuff you captured was actually relevant. so predicting everything to be part of the positive class gives a prefect recall but hurts your precision. and predicting everything to be part of the negative class (returning nothing) gives perfect precision but very bad recall. most models balance these two things.\n",
    "\n",
    "other sources:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling\n",
    "\n",
    "**description:**\n",
    "\n",
    "resampling is a process of estimating how precise or good your statistical measures are usually through the use of some randomization of the underlying data. by randomizing the undrlying data and recomputing your statistics you get an idea for whether or not your original results were random or actually significant. \n",
    "\n",
    "to estimate the precisiion of sample statistics you can use **jackknifing** (taking subsets of the original sample) and **bootstrapping** (drawing randomly with replacement from te original sample). bootstrapping and jacknifing are ways to stimate the parameter distribution for a population from a sample. you can also perform a signifgance test using a **permutation test** (by shuffling labels on data points and seeing if you can repeat your results).\n",
    "\n",
    "**examples:**\n",
    "\n",
    "let's look at the amount of food eaten by stimulated rats and unstimulated rats:\n",
    "\n",
    "```python\n",
    "control = [8,7,4,14,6,7,12,5,5,8]\n",
    "stimulated = [12,7,3,11,8,5,14,7,9,10]\n",
    "\n",
    "std_control = np.std([8,7,4,14,6,7,12,5,5,8])\n",
    "std_stimulated = np.std([12,7,3,11,8,5,14,7,9,10])\n",
    "```\n",
    "\n",
    "$\\sigma_{control} = 3.006$\n",
    "\n",
    "$\\sigma_{stimulated} = 3.136$\n",
    "\n",
    "$\\mu_{control}: 7.5999$\n",
    "\n",
    "$\\mu_{stimulated}: 8.5999$\n",
    "\n",
    "we can use **jacknifing** to estimate whether or not we could have gotten this extreme a standard deviation for stimulated rats by random chance (jackknifing is good for this). \n",
    "\n",
    "```python\n",
    "n = len(stimulated)\n",
    "orig_std = 3.136\n",
    "stds = []\n",
    "for i in range(n):\n",
    "    jackknifed = stimulated[:i]+stimulated[i+1:]\n",
    "    stds.append(np.std(jackknifed))\n",
    "\n",
    "mean_jn = np.mean(stds) # mean sigma value from jacknifes\n",
    "bias_jn = (n-1)*(mean_jn - orig_std) # bias value of original estimate\n",
    "```\n",
    "\n",
    "the $mean_{jn} = 3.109$ is the mean value of $\\sigma$ from all our jacknife rounds. the $bias_jn = 0.000796$  is the amount by which our original estimate $\\sigma_{stimulated}$ overestimates the actual population parameter (or underestimates if negative). the bias is very small and so we can safely say that our original estimate is a pretty good one.\n",
    "\n",
    "we can use **bootstrapping** to generate confidence intervals for the true population parameters. lets generate a 90% confidence interval for the population means.\n",
    "\n",
    "```python\n",
    "n = len(stimulated)\n",
    "bootstrapped_means = []\n",
    "\n",
    "for i in range(100):\n",
    "    bootstrapped = np.random.choice(stimulated, size=n, replace=True)\n",
    "    bootstrapped_means.append(np.mean(bootstrapped))\n",
    "    \n",
    "means = sorted(bootstrapped_means)[5:95]\n",
    "interval = [means[0], means[-1]]\n",
    "```\n",
    "\n",
    "$interval = [6.7999, 10.1999]$\n",
    "\n",
    "we just drew 100 samples from our original stimulated rat sample of the same size as the original sample with replacement (repeats allowed). then we calculated a mean for each of these 100 samples. then we sorted those means and shaved 5% (5 means) off of either side. this is a 90% confidence interval. If we regenerated a confidence interval in thir way 90% of those intervals would contain the true population parameter.\n",
    "\n",
    "we will conduct a **random permutation test** on our two means to see if the result we got could have been by random chance. this is similar to assuming the normal distribution represents our data and finding the p-value; the difference here is that we do not assume any distribution.\n",
    "\n",
    "we have:\n",
    "\n",
    "$H_0: \\mu_{control} = \\mu_{stimulated}$\n",
    "\n",
    "$H_a: \\mu_{control} \\ne \\mu_{stimulated}$ \n",
    "\n",
    "$\\mu_{control}: 7.5999$\n",
    "\n",
    "$\\mu_{stimulated}: 8.5999$\n",
    "\n",
    "there is a difference of 1 between the means. could we have obtained this result by random chance? let's run our permutation test to find out.\n",
    "\n",
    "\n",
    "```python\n",
    "control = [8,7,4,14,6,7,12,5,5,8]\n",
    "stimulated = [12,7,3,11,8,5,14,7,9,10]\n",
    "comb = control + stimulated\n",
    "diffs = []\n",
    "\n",
    "# get bootsrapped differences between means\n",
    "for i in range(100):\n",
    "    control, stimulated = np.split(np.random.permutation(comb), 2)\n",
    "    diffs.append(np.mean(control)-np.mean(stimulated))\n",
    "\n",
    "# count how many of the bootstrapped means are between -1 and 1\n",
    "diffs = np.array(diffs)\n",
    "pvalue = ((diffs < -1).sum() + (diffs > 1).sum())/100\n",
    "```\n",
    "\n",
    "the pvalue should be 0.40-0.50 so 40-50% of the bootstrapped samples are as extreme or more extreme than 1. which means we easily could have gotten a difference of means of 1 by chance. we can say then that getting a difference of means of 1 between $\\mu_{control}$ and $\\mu_{stimulated}$ is not a significant result. and we can reject our alternate hypothesis $H_a$.\n",
    "\n",
    "other sources:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Jackknife_resampling\n",
    "\n",
    "https://www.mathworks.com/examples/statistics/mw/stats-ex23445579-jackknife-resampling\n",
    "\n",
    "https://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works\n",
    "\n",
    "http://www.statisticshowto.com/bootstrap-sample/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
    "\n",
    "http://danielnee.com/2015/01/random-permutation-tests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
