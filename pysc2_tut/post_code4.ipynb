{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create an AI that can play the mineral collection game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "from pysc2.env import sc2_env, run_loop, available_actions_printer, environment\n",
    "from pysc2 import maps\n",
    "from absl import flags\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_AI_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_AI_SELECTED = features.SCREEN_FEATURES.selected.index\n",
    "_AI_SELF = 1\n",
    "_AI_NEUTRAL = 3\n",
    "_NO_OP = actions.FUNCTIONS.no_op.id\n",
    "_MOVE_SCREEN = actions.FUNCTIONS.Attack_screen.id\n",
    "_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n",
    "_SELECT_ALL = [0]\n",
    "_NOT_QUEUED = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe how our AI will see transitions:\n",
    "\n",
    "```\n",
    "state: \n",
    "    marine_selected - a 0 or 1 representing if the marine is selected\n",
    "    marinex_avg - the x position of the middle of the marine\n",
    "    mariney_avg - the y position of the middle of the marine\n",
    "    beaconx_avg - the x position of the middle of the beacon\n",
    "    beacony_avg - the y position of the middle of the beacon\n",
    "    \n",
    "actions:\n",
    "    select - the AI can take the action to select the marine or not select it\n",
    "    movex - once selected the AI can choose to move the marine along the X direction\n",
    "    movey - once selected the AI can choose to move the marine along the Y direction\n",
    "```\n",
    "\n",
    "For example a state might look like this:\n",
    "\n",
    "\n",
    "```\n",
    "(1,5,6,25,25) # selected marine at position (5,6), beacon at (25,25)\n",
    "```\n",
    "\n",
    "And some of the next possible states might look like:\n",
    "\n",
    "```\n",
    "(1,6,7,25,25)\n",
    "(1,5,5,25,25)\n",
    "(1,4,5,25,25)\n",
    "(1,5,7,25,25)\n",
    "(0,5,6,25,25) # unselecting the marine\n",
    "```\n",
    "\n",
    "If our algorithm works right - at the beginning we expect to go from a state like:\n",
    "\n",
    "```\n",
    "(0,x,y,x,y)\n",
    "```\n",
    "\n",
    "To:\n",
    "```\n",
    "(1,x,y,x,y)\n",
    "```\n",
    "\n",
    "Step 1 should be selecting the marine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 128\n",
    "BETA = 0.5\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 500\n",
    "MAX_EPISODES = 100\n",
    "MAX_STEPS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent3(base_agent.BaseAgent):\n",
    "    def step(self, obs):\n",
    "        super(Agent3, self).step(obs)\n",
    "        return decision_function(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.transitions = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def store_transition(self, transition):\n",
    "        # if were below capacity just add the transition\n",
    "        if len(self.transitions) < self.capacity:\n",
    "            self.transitions.append(transition)\n",
    "        # if were above capacity replace transitions randomly\n",
    "        else:\n",
    "            rand_index = np.random.randint(0, len(self.transitions))\n",
    "            self.transitions[rand_index] = transition\n",
    "        \n",
    "    def get_transitions_batches(self, batch_size):\n",
    "        idxs = np.random.choice(len(self.transitions), batch_size)\n",
    "        return np.array(self.transitions)[idxs]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.transitions)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, out_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def get_eps_threshold(steps_done):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "def update_dqn(replay_buf, dqn, optimizer):\n",
    "    if len(replay_buf) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = replay_buf.get_transitions_batches(BATCH_SIZE)\n",
    "    \n",
    "    batch_states = Variable(torch.stack([torch.Tensor(trans[0]) for trans in transitions]))\n",
    "    batch_next_states = Variable(torch.stack([torch.Tensor(trans[1]) for trans in transitions if trans[1]]))\n",
    "    batch_next_states_nonnull = Variable(torch.stack([torch.Tensor(trans[1]) for trans in transitions if trans[1] is not None]))\n",
    "    batch_next_states_nonnull_mask = [i for i, trans in enumerate(transitions) if trans[1] is not None]\n",
    "    batch_actions = Variable(torch.cat([torch.LongTensor([int(trans[2])]) for trans in transitions]))\n",
    "    batch_rewards = Variable(torch.cat([torch.Tensor([int(trans[3])]) for trans in transitions]))\n",
    "    batch_done = Variable(torch.cat([torch.Tensor([int(trans[4])]) for trans in transitions]))\n",
    "    \n",
    "    # get predictions\n",
    "    state_qvalues = dqn(batch_states)\n",
    "    state_qvalues = torch.gather(state_qvalues, 1, batch_actions.view(-1, 1)).view(-1)\n",
    "    # get the actual reward for the next step\n",
    "    next_state_qvalues = Variable(torch.zeros(BATCH_SIZE).type(torch.Tensor))\n",
    "    next_state_qvalues[batch_next_states_nonnull_mask] = dqn(batch_next_states_nonnull).max(1)[0]\n",
    "    print(next_state_qvalues)\n",
    "    print(batch_rewards)\n",
    "    # get the 'labels' to compare our q values against\n",
    "    qvalue_labels = (next_state_qvalues*GAMMA) + batch_rewards\n",
    "    \n",
    "    # calculate a loss and update weights\n",
    "    loss = torch.sum((state_qvalues - qvalue_labels)**2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "    \n",
    "def get_state(obs):\n",
    "    # get the positions of the marine and the beacon\n",
    "    ai_view = obs.observation['screen'][_AI_RELATIVE]\n",
    "    beacon_xs, beacon_ys = (ai_view == _AI_NEUTRAL).nonzero()\n",
    "    beacon_x, beacon_y = beacon_xs.mean(), beacon_ys.mean()\n",
    "    marine_xs, marine_ys = (ai_view == _AI_SELF).nonzero()\n",
    "    marine_x, marine_y = marine_xs.mean(), marine_ys.mean()\n",
    "    \n",
    "    # get a 1 or 0 for whether or not our marine is selected\n",
    "    ai_selected = obs.observation['screen'][_AI_SELECTED]\n",
    "    marine_selected = int((ai_selected == 1).any())\n",
    "    \n",
    "    # return a state that summarizes where the marine is\n",
    "    # and where the beacon is in this world, and whether the marine\n",
    "    # is selected\n",
    "    return (marine_selected, marine_x, marine_y, beacon_x, beacon_y)\n",
    "\n",
    "def get_action(obs, dqn, steps, actiondict):\n",
    "    current_state = get_state(obs)\n",
    "    state_in = Variable(torch.FloatTensor(current_state))\n",
    "    # get q values for potential future states\n",
    "    q_values = dqn(state_in)\n",
    "    # make a little matrix with the outputs that \n",
    "    # represents the q values for the next state\n",
    "    q_values = q_values.data.view(3, 3).numpy()\n",
    "    # takes some random action\n",
    "    thresh = get_eps_threshold(steps)\n",
    "    if np.random.rand() < thresh:\n",
    "        action = np.random.randint(0,9)\n",
    "    # of take the action our dqn chose\n",
    "    else:\n",
    "        action = np.argmax(q_values)\n",
    "    movey, movex = actiondict[str(action)]\n",
    "    destx = max(0, current_state[1]+movex)\n",
    "    desty = max(0, current_state[2]+movey)\n",
    "    # tell our agent to move the marine, swapping position of x and y\n",
    "    return np.argmax(q_values), desty, destx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['run_sc2'])\n",
    "\n",
    "viz = False\n",
    "total_steps = 0\n",
    "save_replay = True\n",
    "episode_rewards = []\n",
    "rp_buf = ReplayBuffer(1000)\n",
    "beacon_map = maps.get('MoveToBeacon')\n",
    "actiondict = {\n",
    "    '0':[0, 0],\n",
    "    '1':[1, 0],\n",
    "    '2':[1, 1],\n",
    "    '3':[0, 1],\n",
    "    '4':[-1, 0],\n",
    "    '5':[-1, -1],\n",
    "    '6':[0, -1],\n",
    "    '7':[1, -1],\n",
    "    '8':[-1, 1]\n",
    "}\n",
    "\n",
    "with sc2_env.SC2Env(agent_race=None, bot_race=None, difficulty=None, map_name=beacon_map, visualize=viz) as env:\n",
    "    # create a dqn, agent\n",
    "    dqn = DQN(5, 9)\n",
    "    optimizer = optim.Adam(dqn.parameters(), lr=LR, betas=(BETA, 0.99))\n",
    "    agent=Agent3()\n",
    "    for i in range(MAX_EPISODES):\n",
    "        ep_reward = 0\n",
    "        ep_reward_extra = 0\n",
    "        obs = env.reset()\n",
    "        for j in range(MAX_STEPS):\n",
    "            total_steps += 1\n",
    "            # select our marine\n",
    "            if _MOVE_SCREEN not in obs[0].observation['available_actions']:\n",
    "                obs = env.step(actions=[actions.FunctionCall(_SELECT_ARMY, [_SELECT_ALL])])\n",
    "            # get the state\n",
    "            state1 = get_state(obs[0])\n",
    "            # take an action in the env\n",
    "            action, desty, destx = get_action(obs[0], dqn, total_steps, actiondict)\n",
    "            func = actions.FunctionCall(_MOVE_SCREEN, [_NOT_QUEUED, [desty, destx]])\n",
    "            obs = env.step(actions=[func])\n",
    "            # get the new state\n",
    "            state2 = get_state(obs[0])\n",
    "            # record the reward and\n",
    "            # whether the episode is done\n",
    "            reward = obs[0].reward\n",
    "            ep_reward += reward\n",
    "#             orig_dist = math.hypot(state1[1] - state1[3], state1[2] - state1[4])\n",
    "#             new_dist = math.hypot(state2[1] - state2[3], state2[2] - state2[4])\n",
    "#             if new_dist < orig_dist:\n",
    "#                 reward += 1\n",
    "            ep_reward_extra += reward\n",
    "            done = int(obs[0].step_type == environment.StepType.LAST)\n",
    "            \n",
    "            if done:\n",
    "                state2 = None\n",
    "            rp_buf.store_transition((state1, state2, action, reward, done))\n",
    "            \n",
    "            if done:\n",
    "                episode_rewards.append(reward)\n",
    "                print('episode_done reward: {}, extra {}'.format(ep_reward, ep_reward_extra))\n",
    "                break\n",
    "            loss = update_dqn(rp_buf, dqn, optimizer)\n",
    "\n",
    "    if save_replay:\n",
    "        env.save_replay(Agent3.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "http://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "https://github.com/xhujoy/pysc2-agents\n",
    "\n",
    "https://arxiv.org/abs/1708.04782"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wron with my model?\n",
    "\n",
    "Does the q function learn, dose the loss go down? Yes\n",
    "\n",
    "\n",
    "Right now the bot just wanders around randomly. there isn't enough reward feedback. So i'll give it a bit of a hint. If it gets closer to the goal ill give it 0.1 reward. this seems ot help a little it actually gets to the goal more than 1x\n",
    "\n",
    "\n",
    "All states are ending up with the same q value close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
