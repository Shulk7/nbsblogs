{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a Q learning agent to learn how to play the move to beacon mini game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "from pysc2.env import sc2_env, run_loop, available_actions_printer\n",
    "from pysc2 import maps\n",
    "from absl import flags\n",
    "\n",
    "_AI_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_AI_SELECTED = features.SCREEN_FEATURES.selected.index\n",
    "_NO_OP = actions.FUNCTIONS.no_op.id\n",
    "_MOVE_SCREEN = actions.FUNCTIONS.Attack_screen.id\n",
    "_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n",
    "_SELECT_POINT = actions.FUNCTIONS.select_point.id\n",
    "_MOVE_RAND = 1000\n",
    "_BACKGROUND = 0\n",
    "_AI_SELF = 1\n",
    "_AI_ALLIES = 2\n",
    "_AI_NEUTRAL = 3\n",
    "_AI_HOSTILE = 4\n",
    "_SELECT_ALL = [0]\n",
    "_NOT_QUEUED = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 2, 12, 1000]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define our actions\n",
    "# it can choose to move to\n",
    "# the beacon or to do nothing\n",
    "# it can select the marine or deselect\n",
    "# the marine, it can move to a random point\n",
    "possible_actions = [\n",
    "    _NO_OP,\n",
    "    _SELECT_ARMY,\n",
    "    _SELECT_POINT,\n",
    "    _MOVE_SCREEN,\n",
    "    _MOVE_RAND\n",
    "]\n",
    "possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 500\n",
    "\n",
    "def get_eps_threshold(steps_done):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "# define the state\n",
    "def get_state(obs):\n",
    "    # get the positions of the marine and the beacon\n",
    "    ai_view = obs.observation['screen'][_AI_RELATIVE]\n",
    "    beaconxs, beaconys = (ai_view == _AI_NEUTRAL).nonzero()\n",
    "    marinexs, marineys = (ai_view == _AI_SELF).nonzero()\n",
    "    marinex, mariney = marinexs.mean(), marineys.mean()\n",
    "        \n",
    "    marine_on_beacon = np.min(beaconxs) <= marinex <=  np.max(beaconxs) and np.min(beaconys) <= mariney <=  np.max(beaconys)\n",
    "        \n",
    "    # get a 1 or 0 for whether or not our marine is selected\n",
    "    ai_selected = obs.observation['screen'][_AI_SELECTED]\n",
    "    marine_selected = int((ai_selected == 1).any())\n",
    "    \n",
    "    return (marine_selected, marine_on_beacon), [beaconxs, beaconys]\n",
    "\n",
    "def get_beacon_location(ai_relative_view):\n",
    "    '''returns the location indices of the beacon on the map'''\n",
    "    return (ai_relative_view == _AI_NEUTRAL).nonzero() \n",
    "\n",
    "class QTable(object):\n",
    "    def __init__(self, actions, lr=0.01, reward_decay=0.9):\n",
    "        self.lr = lr\n",
    "        self.actions = actions\n",
    "        self.reward_decay = reward_decay\n",
    "        self.states_list = set()\n",
    "        self.q_table = np.zeros((0, len(possible_actions))) # create a Q table\n",
    "        \n",
    "    def get_action(self, state, steps):\n",
    "        if np.random.rand() < get_eps_threshold(steps):\n",
    "            return np.random.randint(0, len(self.actions))\n",
    "        else:\n",
    "            if state not in self.states_list:\n",
    "                self.add_state(state)\n",
    "            idx = list(self.states_list).index(state)\n",
    "            q_values = self.q_table[idx]\n",
    "            return int(np.argmax(q_values))\n",
    "    \n",
    "    def add_state(self, state):\n",
    "        self.q_table = np.vstack([self.q_table, np.zeros((1, len(possible_actions)))])\n",
    "        self.states_list.add(state)\n",
    "    \n",
    "    def update_qtable(self, state, next_state, action, reward):\n",
    "        if state not in self.states_list:\n",
    "            self.add_state(state)\n",
    "        if next_state not in self.states_list:\n",
    "            self.add_state(next_state)\n",
    "        # how much reward \n",
    "        state_idx = list(self.states_list).index(state)\n",
    "        next_state_idx = list(self.states_list).index(next_state)\n",
    "        # calculate q labels\n",
    "        q_state = self.q_table[state_idx, action]\n",
    "        q_next_state = self.q_table[next_state_idx].max()\n",
    "        q_targets = reward + (self.reward_decay * q_next_state)\n",
    "        # calculate our loss \n",
    "        loss = q_targets - q_state\n",
    "        # update the q value for this state/action pair\n",
    "        self.q_table[state_idx, action] += self.lr * loss\n",
    "        return loss\n",
    "    \n",
    "    def get_size(self):\n",
    "        print(self.q_table.shape)\n",
    "    \n",
    "class Agent3(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(Agent3, self).__init__()\n",
    "        self.qt = QTable(possible_actions)\n",
    "        \n",
    "    def step(self, obs, steps):\n",
    "        '''Step function gets called automatically by pysc2 environment'''\n",
    "        super(Agent3, self).step(obs)\n",
    "        state, beacon_pos = get_state(obs)\n",
    "        action = self.qt.get_action(state, steps)\n",
    "        func = actions.FunctionCall(_NO_OP, [])\n",
    "        \n",
    "        if possible_actions[action] == _NO_OP:\n",
    "            func = actions.FunctionCall(_NO_OP, [])\n",
    "        elif state[0] and possible_actions[action] == _MOVE_SCREEN:\n",
    "            beacon_x, beacon_y = beacon_pos[0].mean(), beacon_pos[1].mean()\n",
    "            func = actions.FunctionCall(_MOVE_SCREEN, [_NOT_QUEUED, [beacon_y, beacon_x]])\n",
    "        elif possible_actions[action] == _SELECT_ARMY:\n",
    "            func = actions.FunctionCall(_SELECT_ARMY, [_SELECT_ALL])\n",
    "        elif state[0] and possible_actions[action] == _SELECT_POINT:\n",
    "            ai_view = obs.observation['screen'][_AI_RELATIVE]\n",
    "            backgroundxs, backgroundys = (ai_view == _BACKGROUND).nonzero()\n",
    "            point = np.random.randint(0, len(backgroundxs))\n",
    "            backgroundx, backgroundy = backgroundxs[point], backgroundys[point]\n",
    "            func = actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, [backgroundy, backgroundx]])\n",
    "        elif state[0] and possible_actions[action] == _MOVE_RAND:\n",
    "            # move somewhere that is not the beacon\n",
    "            beacon_x, beacon_y = beacon_pos[0].max(), beacon_pos[1].max()\n",
    "            movex, movey = np.random.randint(beacon_x, 64), np.random.randint(beacon_y, 64)\n",
    "            func = actions.FunctionCall(_MOVE_SCREEN, [_NOT_QUEUED, [movey, movex]])\n",
    "        return state, action, func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yvanscher/anaconda/envs/pysc2/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "/Users/yvanscher/anaconda/envs/pysc2/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward: 12, Explore threshold: 0.4319296194996383, Q loss: 0.023910463865926202\n",
      "Starting episode.\n",
      "Episode Reward: 11, Explore threshold: 0.22161204029545706, Q loss: 0.04142643829312305\n",
      "Starting episode.\n",
      "Episode Reward: 3, Explore threshold: 0.12711026029600064, Q loss: -0.005607384205412298\n",
      "Starting episode.\n",
      "Episode Reward: 3, Explore threshold: 0.08464787338161128, Q loss: -0.005887233744322706\n",
      "Starting episode.\n",
      "Episode Reward: 4, Explore threshold: 0.06556829305542405, Q loss: -0.006717998007231038\n",
      "Starting episode.\n",
      "Episode Reward: 3, Explore threshold: 0.056995284991667026, Q loss: -0.006327417101438297\n",
      "Starting episode.\n",
      "Episode Reward: 1, Explore threshold: 0.053143184159010495, Q loss: -0.0030889159882449904\n",
      "Starting episode.\n",
      "Episode Reward: 3, Explore threshold: 0.05141232368219785, Q loss: -0.004695781222213362\n",
      "Starting episode.\n",
      "Episode Reward: 1, Explore threshold: 0.050634597937120183, Q loss: -0.0035450500421394596\n",
      "Starting episode.\n",
      "Episode Reward: 2, Explore threshold: 0.05028514323371714, Q loss: -0.004024226463889995\n",
      "Starting episode.\n",
      "Episode Reward: 1, Explore threshold: 0.050128123113831156, Q loss: -0.0036818566592185936\n",
      "Starting episode.\n",
      "Episode Reward: 6, Explore threshold: 0.05005756942601723, Q loss: -0.0055104426459602965\n",
      "Starting episode.\n",
      "Episode Reward: 2, Explore threshold: 0.05002586761055715, Q loss: -0.0034559906708881982\n",
      "Starting episode.\n",
      "Episode Reward: 4, Explore threshold: 0.05001162306665583, Q loss: -0.006190330676054408\n",
      "Starting episode.\n",
      "Episode Reward: 2, Explore threshold: 0.05000522258050033, Q loss: -0.0027704190152995044\n",
      "Starting episode.\n",
      "Episode Reward: 2, Explore threshold: 0.050002346656686236, Q loss: -0.006000523408501518\n",
      "Starting episode.\n",
      "Episode Reward: 1, Explore threshold: 0.05000105442081797, Q loss: -0.00465772935936988\n",
      "Starting episode.\n",
      "Episode Reward: 1, Explore threshold: 0.05000047378181388, Q loss: -0.002517767866574625\n",
      "Starting episode.\n",
      "Episode Reward: 1, Explore threshold: 0.05000021288389165, Q loss: -0.004272236083964272\n",
      "Starting episode.\n",
      "Episode Reward: 2, Explore threshold: 0.05000009565489851, Q loss: -0.0035897523409358922\n",
      "Starting episode.\n",
      "Episode Reward: 3, Explore threshold: 0.05000004298051646, Q loss: -0.0033554137560508034\n",
      "Starting episode.\n",
      "Episode Reward: 36, Explore threshold: 0.05000001931239094, Q loss: -0.01986586174692806\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05000000867761662, Q loss: -0.023299795812754043\n",
      "Starting episode.\n",
      "Episode Reward: 41, Explore threshold: 0.050000003899104487, Q loss: -0.03003743342608456\n",
      "Starting episode.\n",
      "Episode Reward: 47, Explore threshold: 0.050000001751980584, Q loss: -0.04486588583629886\n",
      "Starting episode.\n",
      "Episode Reward: 45, Explore threshold: 0.05000000078721562, Q loss: -0.04865220127449682\n",
      "Starting episode.\n",
      "Episode Reward: 41, Explore threshold: 0.05000000035371878, Q loss: -0.05431036543658252\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.050000000158936096, Q loss: -0.05590609230173316\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05000000007141459, Q loss: -0.05836230997071312\n",
      "Starting episode.\n",
      "Episode Reward: 45, Explore threshold: 0.05000000003208865, Q loss: -0.06148672071781536\n",
      "Starting episode.\n",
      "Episode Reward: 40, Explore threshold: 0.05000000001441836, Q loss: -0.06438515720094995\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05000000000647859, Q loss: -0.06986481768562769\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05000000000291102, Q loss: -0.06987877833550704\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.050000000001308005, Q loss: -0.06950780369091525\n",
      "Starting episode.\n",
      "Episode Reward: 46, Explore threshold: 0.05000000000058773, Q loss: 0.9270289858936812\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05000000000026408, Q loss: -0.07342885824754097\n",
      "Starting episode.\n",
      "Episode Reward: 38, Explore threshold: 0.050000000000118665, Q loss: -0.07399075894470819\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05000000000005332, Q loss: -0.07730826245113576\n",
      "Starting episode.\n",
      "Episode Reward: 46, Explore threshold: 0.05000000000002396, Q loss: -0.08170714448446526\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.050000000000010765, Q loss: 0.46379168352607925\n",
      "Starting episode.\n",
      "Episode Reward: 45, Explore threshold: 0.05000000000000484, Q loss: -0.08645642945394039\n",
      "Starting episode.\n",
      "Episode Reward: 46, Explore threshold: 0.050000000000002175, Q loss: 0.42979487115451726\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05000000000000098, Q loss: -0.08912064904423478\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05000000000000044, Q loss: -0.09014839406054287\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.0500000000000002, Q loss: -0.09072138673146068\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05000000000000009, Q loss: -0.09032557232593896\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.050000000000000044, Q loss: -0.09094606856520271\n",
      "Starting episode.\n",
      "Episode Reward: 39, Explore threshold: 0.050000000000000024, Q loss: -0.08819345514078869\n",
      "Starting episode.\n",
      "Episode Reward: 45, Explore threshold: 0.05000000000000001, Q loss: -0.09184105704023471\n",
      "Starting episode.\n",
      "Episode Reward: 40, Explore threshold: 0.05000000000000001, Q loss: -0.09051520580494865\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05, Q loss: 0.08817532130273187\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05, Q loss: -0.09356354025438474\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05, Q loss: -0.09351326573801899\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05, Q loss: 0.9065891967638433\n",
      "Starting episode.\n",
      "Episode Reward: 40, Explore threshold: 0.05, Q loss: -0.09327514192791841\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05, Q loss: -0.09439071021479517\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: -0.09629559741105875\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05, Q loss: -0.0949407535766078\n",
      "Starting episode.\n",
      "Episode Reward: 39, Explore threshold: 0.05, Q loss: -0.09413613707899093\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05, Q loss: -0.0942977882755538\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: -0.09730278822754534\n",
      "Starting episode.\n",
      "Episode Reward: 48, Explore threshold: 0.05, Q loss: -0.10182889498931458\n",
      "Starting episode.\n",
      "Episode Reward: 41, Explore threshold: 0.05, Q loss: -0.09686661249803996\n",
      "Starting episode.\n",
      "Episode Reward: 39, Explore threshold: 0.05, Q loss: -0.09675045770913127\n",
      "Starting episode.\n",
      "Episode Reward: 41, Explore threshold: 0.05, Q loss: -0.09586244859409265\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05, Q loss: 0.05026292211148675\n",
      "Starting episode.\n",
      "Episode Reward: 46, Explore threshold: 0.05, Q loss: 0.23157092539323598\n",
      "Starting episode.\n",
      "Episode Reward: 41, Explore threshold: 0.05, Q loss: -0.09859357000950375\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05, Q loss: -0.10031133222705246\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05, Q loss: 0.900949086698214\n",
      "Starting episode.\n",
      "Episode Reward: 45, Explore threshold: 0.05, Q loss: 0.9000189699253728\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05, Q loss: -0.09951016464237816\n",
      "Starting episode.\n",
      "Episode Reward: 39, Explore threshold: 0.05, Q loss: -0.09614979868004203\n",
      "Starting episode.\n",
      "Episode Reward: 39, Explore threshold: 0.05, Q loss: 0.9068611511689739\n",
      "Starting episode.\n",
      "Episode Reward: 46, Explore threshold: 0.05, Q loss: -0.0987411860774996\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: -0.10041425800898218\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: -0.10063252309734627\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: -0.10188126503895678\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05, Q loss: -0.10234882380461341\n",
      "Starting episode.\n",
      "Episode Reward: 40, Explore threshold: 0.05, Q loss: 0.5733780164218134\n",
      "Starting episode.\n",
      "Episode Reward: 45, Explore threshold: 0.05, Q loss: -0.10196375674010327\n",
      "Starting episode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: -0.10352868613317101\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: 0.8983357588190155\n",
      "Starting episode.\n",
      "Episode Reward: 39, Explore threshold: 0.05, Q loss: -0.0997603639414234\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: 0.163971703223458\n",
      "Starting episode.\n",
      "Episode Reward: 40, Explore threshold: 0.05, Q loss: -0.09902019651432437\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05, Q loss: -0.10034731764565863\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05, Q loss: -0.10082770033242994\n",
      "Starting episode.\n",
      "Episode Reward: 39, Explore threshold: 0.05, Q loss: -0.09842674104476334\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: 0.9014751272764222\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05, Q loss: -0.09921306803691898\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: -0.10062659139479413\n",
      "Starting episode.\n",
      "Episode Reward: 44, Explore threshold: 0.05, Q loss: -0.10229753494805793\n",
      "Starting episode.\n",
      "Episode Reward: 42, Explore threshold: 0.05, Q loss: -0.10228798719059207\n",
      "Starting episode.\n",
      "Episode Reward: 40, Explore threshold: 0.05, Q loss: -0.09887452441411682\n",
      "Starting episode.\n",
      "Episode Reward: 39, Explore threshold: 0.05, Q loss: -0.0984154588986802\n",
      "Starting episode.\n",
      "Episode Reward: 45, Explore threshold: 0.05, Q loss: -0.10154173548181433\n",
      "Starting episode.\n",
      "Episode Reward: 43, Explore threshold: 0.05, Q loss: -0.10155567569804624\n",
      "Starting episode.\n",
      "Episode Reward: 46, Explore threshold: 0.05, Q loss: 0.8987337640847388\n",
      "Starting episode.\n",
      "Episode Reward: 45, Explore threshold: 0.05, Q loss: -0.10315564579441039\n"
     ]
    }
   ],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['run_sc2'])\n",
    "\n",
    "viz = False\n",
    "save_replay = False\n",
    "steps_per_episode = 0 # 0 actually means unlimited\n",
    "MAX_EPISODES = 100\n",
    "MAX_STEPS = 400\n",
    "total_steps = 0\n",
    "\n",
    "# create a map\n",
    "beacon_map = maps.get('MoveToBeacon')\n",
    "\n",
    "# create an envirnoment\n",
    "with sc2_env.SC2Env(agent_race=None,\n",
    "                    bot_race=None,\n",
    "                    difficulty=None,\n",
    "                    map_name=beacon_map,\n",
    "                    visualize=viz) as env:\n",
    "    agent = Agent3()\n",
    "    for i in range(MAX_EPISODES):\n",
    "        print('Starting episode.')\n",
    "        ep_reward = 0\n",
    "        obs = env.reset()\n",
    "        for j in range(MAX_STEPS):\n",
    "            total_steps += 1\n",
    "            state, action, func = agent.step(obs[0], total_steps)\n",
    "            obs = env.step(actions=[func])\n",
    "            next_state, _ = get_state(obs[0])\n",
    "            reward = obs[0].reward\n",
    "            ep_reward += reward\n",
    "            loss = agent.qt.update_qtable(state, next_state, action, reward)\n",
    "        print('Episode Reward: {}, Explore threshold: {}, Q loss: {}'.format(ep_reward, get_eps_threshold(total_steps), loss))\n",
    "    if save_replay:\n",
    "        env.save_replay(Agent3.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.270000000000003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.reward/MAX_EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.qt.q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed our Q learning agent actually outperforms an agent that is told to simple move to the beacon? How is that possible? Let's examine the Q Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, False) [ 0.892184    0.95317795  0.89820497  1.0305249   0.84618695]\n",
      "(0, False) [ 0.26923404  1.74120036  0.09948828  0.18952466  0.21669156]\n",
      "(1, True) [ 0.15577372  1.90523865  0.11505073  0.22864302  0.15269515]\n"
     ]
    }
   ],
   "source": [
    "for state in agent.qt.states_list:\n",
    "    print(state, agent.qt.q_table[list(agent.qt.states_list).index(state)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when it has the marine selected but its not at the beacon, state=(1, False), our agent learns that moving to the beacon has the highest value 1.03 (action at index 3).\n",
    "\n",
    "When it doesnt have the marine selected and its not at the beacon, state=(0,False), our agent learns to select the marine has the highest value 1.74 (action at index 1).\n",
    "\n",
    "When it is one the beacon and it has the marine selected, state=(1,True) it learns that reselecting, continuing to select the marine gives the highest reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structuring the porblem is very important. Originally I had a different state representation and it didn't learn anything (much fo anything).  had represented the state as the position of the beacon and the position of the marine but there were so many states and the Q learning agent didn't have any function that could tell it \"here is what those locations mean.\"\n",
    "\n",
    "I didn't want to teach it to recognize the beacon as well as move to the beacon so I just taught it to move to beacon as a block decision. If it chooses to move to the beacon\n",
    "\n",
    "Our bot can choose to do 2 things. It can choose to just sit there and do _NO_OP or it canmove to the beacon. It has to learn to move to the beacon.\n",
    "\n",
    "It would be nice to also teach it to recognize the becaon, let's examie this next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
