{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a Q learning agent to learn how to play the move to beacon mini game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "from pysc2.env import sc2_env, run_loop, available_actions_printer\n",
    "from pysc2 import maps\n",
    "from absl import flags\n",
    "\n",
    "_AI_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_AI_SELECTED = features.SCREEN_FEATURES.selected.index\n",
    "_NO_OP = actions.FUNCTIONS.no_op.id\n",
    "_MOVE_SCREEN = actions.FUNCTIONS.Attack_screen.id\n",
    "_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n",
    "_SELECT_POINT = actions.FUNCTIONS.select_point.id\n",
    "_MOVE_RAND = 1000\n",
    "_MOVE_MIDDLE = 2000\n",
    "_BACKGROUND = 0\n",
    "_AI_SELF = 1\n",
    "_AI_ALLIES = 2\n",
    "_AI_NEUTRAL = 3\n",
    "_AI_HOSTILE = 4\n",
    "_SELECT_ALL = [0]\n",
    "_NOT_QUEUED = [0]\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.025\n",
    "EPS_DECAY = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 2, 12, 1000, 2000]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define our actions\n",
    "# it can choose to move to\n",
    "# the beacon or to do nothing\n",
    "# it can select the marine or deselect\n",
    "# the marine, it can move to a random point\n",
    "possible_actions = [\n",
    "    _NO_OP,\n",
    "    _SELECT_ARMY,\n",
    "    _SELECT_POINT,\n",
    "    _MOVE_SCREEN,\n",
    "    _MOVE_RAND,\n",
    "    _MOVE_MIDDLE\n",
    "]\n",
    "possible_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're giving our agent the ability to do 6 things. \n",
    "\n",
    "`_NO_OP` - do nothing\n",
    "\n",
    "`_SELECT_ARMY` - select the marine\n",
    "\n",
    "`__SELECT_POINT` - deselect the marine\n",
    "\n",
    "`_MOVE_SCREEN` - move to the beacon\n",
    "\n",
    "`_MOVERAND` - move to a random point that is not the beacon\n",
    "\n",
    "`_MOVE_MIDDLE` - move to a point that is in the middle of the map\n",
    "\n",
    "For our Q learning table we're not going to teach our agent to recognize the beacon itself as this is a bit more complex. For now we just want it to realize that there are 6 things it can do in this world and that there is a sequence of some f those actions which produces a positive feedback / reward.\n",
    "\n",
    "Let's examine what our agent can see about the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eps_threshold(steps_done):\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "# define the state\n",
    "def get_state(obs):\n",
    "    # get the positions of the marine and the beacon\n",
    "    ai_view = obs.observation['screen'][_AI_RELATIVE]\n",
    "    beaconxs, beaconys = (ai_view == _AI_NEUTRAL).nonzero()\n",
    "    marinexs, marineys = (ai_view == _AI_SELF).nonzero()\n",
    "    marinex, mariney = marinexs.mean(), marineys.mean()\n",
    "        \n",
    "    marine_on_beacon = np.min(beaconxs) <= marinex <=  np.max(beaconxs) and np.min(beaconys) <= mariney <=  np.max(beaconys)\n",
    "        \n",
    "    # get a 1 or 0 for whether or not our marine is selected\n",
    "    ai_selected = obs.observation['screen'][_AI_SELECTED]\n",
    "    marine_selected = int((ai_selected == 1).any())\n",
    "    \n",
    "    return (marine_selected, int(marine_on_beacon)), [beaconxs, beaconys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Agent can see the world like this:\n",
    "\n",
    "`(1 | 0, 1 | 0)`\n",
    "\n",
    "In other words our agent knows if we selected the marine and if the marine is current on the beacon. There are only 4 possible states:\n",
    "\n",
    "`(0, 0)` - marine not selected, marine not on the beacon.\n",
    "\n",
    "`(1, 0)` - marine selected but not on the beacon.\n",
    "\n",
    "`(1, 1)` - marine selected and is also on the becaon.\n",
    "\n",
    "`(0, 1)` - marine not selected but is on the beacon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable(object):\n",
    "    def __init__(self, actions, lr=0.01, reward_decay=0.9, load_qt=None, load_st=None):\n",
    "        self.lr = lr\n",
    "        self.actions = actions\n",
    "        self.reward_decay = reward_decay\n",
    "        self.states_list = set()\n",
    "        self.load_qt = load_qt\n",
    "        if load_st:\n",
    "            temp = self.load_states(load_st)\n",
    "            self.states_list = set([tuple(temp[i]) for i in range(len(temp))])\n",
    "        \n",
    "        if load_qt:\n",
    "            self.q_table = self.load_qtable(load_qt)\n",
    "        else:\n",
    "            self.q_table = np.zeros((0, len(possible_actions))) # create a Q table\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if not self.load_qt and np.random.rand() < get_eps_threshold(steps):\n",
    "            return np.random.randint(0, len(self.actions))\n",
    "        else:\n",
    "            if state not in self.states_list:\n",
    "                self.add_state(state)\n",
    "            idx = list(self.states_list).index(state)\n",
    "            q_values = self.q_table[idx]\n",
    "            return int(np.argmax(q_values))\n",
    "    \n",
    "    def add_state(self, state):\n",
    "        self.q_table = np.vstack([self.q_table, np.zeros((1, len(possible_actions)))])\n",
    "        self.states_list.add(state)\n",
    "    \n",
    "    def update_qtable(self, state, next_state, action, reward):\n",
    "        if state not in self.states_list:\n",
    "            self.add_state(state)\n",
    "        if next_state not in self.states_list:\n",
    "            self.add_state(next_state)\n",
    "        # how much reward \n",
    "        state_idx = list(self.states_list).index(state)\n",
    "        next_state_idx = list(self.states_list).index(next_state)\n",
    "        # calculate q labels\n",
    "        q_state = self.q_table[state_idx, action]\n",
    "        q_next_state = self.q_table[next_state_idx].max()\n",
    "        q_targets = reward + (self.reward_decay * q_next_state)\n",
    "        # calculate our loss \n",
    "        loss = q_targets - q_state\n",
    "        # update the q value for this state/action pair\n",
    "        self.q_table[state_idx, action] += self.lr * loss\n",
    "        return loss\n",
    "    \n",
    "    def get_size(self):\n",
    "        print(self.q_table.shape)\n",
    "        \n",
    "    def save_qtable(self, filepath):\n",
    "        np.save(filepath, self.q_table)\n",
    "        \n",
    "    def load_qtable(self, filepath):\n",
    "        return np.load(filepath)\n",
    "        \n",
    "    def save_states(self, filepath):\n",
    "        temp = np.array(list(self.states_list))\n",
    "        np.save(filepath, temp)\n",
    "        \n",
    "    def load_states(self, filepath):\n",
    "        return np.load(filepath)\n",
    "    \n",
    "class Agent3(base_agent.BaseAgent):\n",
    "    def __init__(self, load_qt=None, load_st=None):\n",
    "        super(Agent3, self).__init__()\n",
    "        self.qtable = QTable(possible_actions, load_qt=load_qt, load_st=load_st)\n",
    "        \n",
    "    def step(self, obs):\n",
    "        '''Step function gets called automatically by pysc2 environment'''\n",
    "        super(Agent3, self).step(obs)\n",
    "        state, beacon_pos = get_state(obs)\n",
    "        action = self.qtable.get_action(state)\n",
    "        func = actions.FunctionCall(_NO_OP, [])\n",
    "        \n",
    "        if possible_actions[action] == _NO_OP:\n",
    "            func = actions.FunctionCall(_NO_OP, [])\n",
    "        elif state[0] and possible_actions[action] == _MOVE_SCREEN:\n",
    "            beacon_x, beacon_y = beacon_pos[0].mean(), beacon_pos[1].mean()\n",
    "            func = actions.FunctionCall(_MOVE_SCREEN, [_NOT_QUEUED, [beacon_y, beacon_x]])\n",
    "        elif possible_actions[action] == _SELECT_ARMY:\n",
    "            func = actions.FunctionCall(_SELECT_ARMY, [_SELECT_ALL])\n",
    "        elif state[0] and possible_actions[action] == _SELECT_POINT:\n",
    "            ai_view = obs.observation['screen'][_AI_RELATIVE]\n",
    "            backgroundxs, backgroundys = (ai_view == _BACKGROUND).nonzero()\n",
    "            point = np.random.randint(0, len(backgroundxs))\n",
    "            backgroundx, backgroundy = backgroundxs[point], backgroundys[point]\n",
    "            func = actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, [backgroundy, backgroundx]])\n",
    "        elif state[0] and possible_actions[action] == _MOVE_RAND:\n",
    "            # move somewhere that is not the beacon\n",
    "            beacon_x, beacon_y = beacon_pos[0].max(), beacon_pos[1].max()\n",
    "            movex, movey = np.random.randint(beacon_x, 64), np.random.randint(beacon_y, 64)\n",
    "            func = actions.FunctionCall(_MOVE_SCREEN, [_NOT_QUEUED, [movey, movex]])\n",
    "        elif state[0] and possible_actions[action] == _MOVE_MIDDLE:\n",
    "            func = actions.FunctionCall(_MOVE_SCREEN, [_NOT_QUEUED, [32, 32]])\n",
    "        return state, action, func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yvanscher/anaconda/envs/pysc2/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/yvanscher/anaconda/envs/pysc2/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward: 1, Explore threshold: 0.770625815345435, Q loss: 6.112720181083372e-05\n",
      "Starting episode 1\n",
      "Episode Reward: 0, Explore threshold: 0.6603804074394796, Q loss: -1.5116670143063867e-05\n",
      "Starting episode 2\n",
      "Episode Reward: 1, Explore threshold: 0.5664354678303732, Q loss: -2.1292038214325244e-05\n",
      "Starting episode 3\n",
      "Episode Reward: 1, Explore threshold: 0.4863808710376675, Q loss: 0.007672810393025457\n",
      "Starting episode 4\n",
      "Episode Reward: 1, Explore threshold: 0.4181628436025689, Q loss: -0.0007654517143376926\n",
      "Starting episode 5\n",
      "Episode Reward: 2, Explore threshold: 0.3600312752282231, Q loss: -0.0006237272210723654\n",
      "Starting episode 6\n",
      "Episode Reward: 0, Explore threshold: 0.31049482029515957, Q loss: -0.0004806166707503664\n",
      "Starting episode 7\n",
      "Episode Reward: 8, Explore threshold: 0.2682826378965449, Q loss: -0.004313301649580499\n",
      "Starting episode 8\n",
      "Episode Reward: 40, Explore threshold: 0.23231178884685655, Q loss: -0.014309225465330505\n",
      "Starting episode 9\n",
      "Episode Reward: 26, Explore threshold: 0.20165945324532344, Q loss: -0.017889587915344646\n",
      "Starting episode 10\n",
      "Episode Reward: 37, Explore threshold: 0.17553925584516922, Q loss: -0.028346580186610515\n",
      "Starting episode 11\n",
      "Episode Reward: 34, Explore threshold: 0.1532810918640564, Q loss: 1.2252712839991156\n",
      "Starting episode 12\n",
      "Episode Reward: 38, Explore threshold: 0.1343139356737596, Q loss: 1.114828611347326\n",
      "Starting episode 13\n",
      "Episode Reward: 37, Explore threshold: 0.1181511913318462, Q loss: -0.03679287458326164\n",
      "Starting episode 14\n",
      "Episode Reward: 24, Explore threshold: 0.10437820912823595, Q loss: -0.04306727390377063\n",
      "Starting episode 15\n",
      "Episode Reward: 37, Explore threshold: 0.09264164788788729, Q loss: 1.3993018633194105\n",
      "Starting episode 16\n",
      "Episode Reward: 33, Explore threshold: 0.08264041012310258, Q loss: -0.05566379785413089\n",
      "Starting episode 17\n",
      "Episode Reward: 28, Explore threshold: 0.07411791747986701, Q loss: -0.060714671040189905\n",
      "Starting episode 18\n",
      "Episode Reward: 36, Explore threshold: 0.06685552830742358, Q loss: -0.06073656659924165\n",
      "Starting episode 19\n",
      "Episode Reward: 27, Explore threshold: 0.060666928481070435, Q loss: -0.057206724233054396\n",
      "Starting episode 20\n",
      "Episode Reward: 29, Explore threshold: 0.055393351576646245, Q loss: -0.06007486670196993\n",
      "Starting episode 21\n",
      "Episode Reward: 8, Explore threshold: 0.0508995057719055, Q loss: -0.03364092668527813\n",
      "Starting episode 22\n",
      "Episode Reward: 18, Explore threshold: 0.047070102980823814, Q loss: -0.060102560099849156\n",
      "Starting episode 23\n",
      "Episode Reward: 14, Explore threshold: 0.04380690117695368, Q loss: -0.0562127943852988\n",
      "Starting episode 24\n",
      "Episode Reward: 32, Explore threshold: 0.04102618402764241, Q loss: -0.058778031877148695\n",
      "Starting episode 25\n",
      "Episode Reward: 24, Explore threshold: 0.03865661317998498, Q loss: -0.061664387804367116\n",
      "Starting episode 26\n",
      "Episode Reward: 0, Explore threshold: 0.0366373980996383, Q loss: -0.026119896592385144\n",
      "Starting episode 27\n",
      "Episode Reward: 9, Explore threshold: 0.034916736510333966, Q loss: -0.05892626842892912\n",
      "Starting episode 28\n",
      "Episode Reward: 33, Explore threshold: 0.03345048542409555, Q loss: 0.9392895508813128\n",
      "Starting episode 29\n",
      "Episode Reward: 22, Explore threshold: 0.032201028667892526, Q loss: -0.06318365555431027\n",
      "Starting episode 30\n",
      "Episode Reward: 23, Explore threshold: 0.031136311853512247, Q loss: -0.023107359000021532\n",
      "Starting episode 31\n",
      "Episode Reward: 30, Explore threshold: 0.030229020033130202, Q loss: -0.06830387582643649\n",
      "Starting episode 32\n",
      "Episode Reward: 9, Explore threshold: 0.029455876943611792, Q loss: -0.06730766974549784\n",
      "Starting episode 33\n",
      "Episode Reward: 43, Explore threshold: 0.028797047861896533, Q loss: -0.06863870635113545\n",
      "Starting episode 34\n",
      "Episode Reward: 43, Explore threshold: 0.028235630751922567, Q loss: -0.06328720080315176\n"
     ]
    }
   ],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['run_sc2'])\n",
    "\n",
    "viz = False\n",
    "save_replay = False\n",
    "steps_per_episode = 0 # 0 actually means unlimited\n",
    "MAX_EPISODES =35\n",
    "MAX_STEPS = 400\n",
    "steps = 0\n",
    "\n",
    "# create a map\n",
    "beacon_map = maps.get('MoveToBeacon')\n",
    "\n",
    "# create an envirnoment\n",
    "with sc2_env.SC2Env(agent_race=None,\n",
    "                    bot_race=None,\n",
    "                    difficulty=None,\n",
    "                    map_name=beacon_map,\n",
    "                    visualize=viz) as env:\n",
    "    agent = Agent3()\n",
    "    for i in range(MAX_EPISODES):\n",
    "        print('Starting episode {}'.format(i))\n",
    "        ep_reward = 0\n",
    "        obs = env.reset()\n",
    "        for j in range(MAX_STEPS):\n",
    "            steps += 1\n",
    "            state, action, func = agent.step(obs[0])\n",
    "            obs = env.step(actions=[func])\n",
    "            next_state, _ = get_state(obs[0])\n",
    "            reward = obs[0].reward\n",
    "            ep_reward += reward\n",
    "            loss = agent.qtable.update_qtable(state, next_state, action, reward)\n",
    "        print('Episode Reward: {}, Explore threshold: {}, Q loss: {}'.format(ep_reward, get_eps_threshold(steps), loss))\n",
    "    if save_replay:\n",
    "        env.save_replay(Agent3.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.257142857142856"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.reward/MAX_EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.qtable.q_table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent learns (slowly at first) and if youwatch it play it kind of meanders around and does a lot of `_NO_OP` (nothing) operations. Somewhere around episode 10 (sometimes its faster or slower to converge anywhere from episode 2 to episode 30) though our Agent learns that going to the beacon is good and starts to move towards it consistently.\n",
    "\n",
    "You may have noticed our Q learning agent actually outperforms an agent that is told to simply move to the beacon. How is that possible? Let's examine the Q Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(marine_sel, marine_beac) [do nothing, select marine, deselect marine, move beacon, move random, move middle] \n",
      "(1, 0) [ 0.25143407  0.28523599  0.28281235  0.63223914  0.21524341  0.26161219]\n",
      "(0, 0) [ 0.093577    0.50350932  0.09130667  0.18796636  0.11642416  0.10940728]\n",
      "(1, 1) [ 1.39015192  0.13389986  0.11158669  0.06626444  0.04365226  0.01829639]\n"
     ]
    }
   ],
   "source": [
    "print('(marine_sel, marine_beac)', '[do nothing, select marine, deselect marine, move beacon, move random, move middle] ')\n",
    "for state in agent.qtable.states_list:\n",
    "    print(state, agent.qtable.q_table[list(agent.qtable.states_list).index(state)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when it has the marine selected but its not at the beacon, state=`(1, 0)`, our agent learns that moving to the beacon has the highest value (action at index 3).\n",
    "\n",
    "When it doesnt have the marine selected and its not at the beacon, state=`(0,0)`, our agent learns to select the marine has the highest value (action at index 1).\n",
    "\n",
    "When it is one the beacon and it has the marine selected, state=`(1,1)`, reselecting the marine is valuable. I've also had it learn that deselection is good here. Sometimes that's going to happen. Our reward function doesn't seem to be affected much by intermittent deselection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save qtable\n",
    "agent.qtable.save_qtable('agent3_qtable.npy')\n",
    "agent.qtable.save_states('agent3_states.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It would be nice to also teach our agent to move to the beacon AND recognize it, let's examine this next. \n",
    "\n",
    "Check this tutorial for a more advanced tabular Q learning agent: https://chatbotslife.com/building-a-smart-pysc2-agent-cdc269cb095d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('agent3_states.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
